{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67c60748",
   "metadata": {},
   "source": [
    "# RAG System untuk Teacher Agent - Automated Feedback System\n",
    "\n",
    "Sistem Retrieval-Augmented Generation (RAG) untuk mendukung Teacher Agent dalam memberikan feedback otomatis kepada siswa.\n",
    "\n",
    "## Pipeline Overview:\n",
    "1. **Data Loading**: Membaca file .txt dari folder raw_data\n",
    "2. **Preprocessing**: Cleaning dan normalisasi teks\n",
    "3. **Chunking**: Strategi chunking dengan overlap\n",
    "4. **Embedding**: Menggunakan model Qwen3-Embedding\n",
    "5. **Vector Store**: FAISS untuk penyimpanan dan pencarian efisien\n",
    "6. **Retrieval**: Similarity search dengan top-k\n",
    "7. **Context Injection**: Format output untuk agent downstream"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c918f89f",
   "metadata": {},
   "source": [
    "## 1. Import Libraries dan Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "931983ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Libraries imported successfully!\n",
      "üîß PyTorch version: 2.9.1\n",
      "üîß CUDA available: False\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import glob\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Tuple\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# LangChain imports\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import TextLoader, DirectoryLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_core.documents import Document\n",
    "# Utility imports\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")\n",
    "print(f\"üîß PyTorch version: {torch.__version__}\")\n",
    "print(f\"üîß CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248e0810",
   "metadata": {},
   "source": [
    "## 2. Data Loading - Membaca File .txt dari raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf16dc57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Found 2 .txt files in raw_data\n",
      "  ‚úì Loaded: raw_data/question_bank/uts2.txt\n",
      "  ‚úì Loaded: raw_data/question_bank/uts1.txt\n",
      "\n",
      "‚è±Ô∏è Loading time: 0.01 seconds\n",
      "üìÑ Total documents loaded: 2\n"
     ]
    }
   ],
   "source": [
    "class DataLoader:\n",
    "    \"\"\"\n",
    "    Custom Data Loader untuk membaca file .txt dari folder raw_data\n",
    "    \"\"\"\n",
    "    def __init__(self, data_dir: str = \"raw_data\"):\n",
    "        self.data_dir = data_dir\n",
    "        self.documents = []\n",
    "        \n",
    "    def load_documents(self) -> List[Document]:\n",
    "        \"\"\"\n",
    "        Load semua file .txt dari direktori raw_data\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Cari semua file .txt secara rekursif\n",
    "        txt_files = glob.glob(os.path.join(self.data_dir, \"**/*.txt\"), recursive=True)\n",
    "        \n",
    "        print(f\"üìÇ Found {len(txt_files)} .txt files in {self.data_dir}\")\n",
    "        \n",
    "        for file_path in txt_files:\n",
    "            try:\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    content = f.read()\n",
    "                    \n",
    "                # Create Document object dengan metadata\n",
    "                doc = Document(\n",
    "                    page_content=content,\n",
    "                    metadata={\n",
    "                        \"source\": file_path,\n",
    "                        \"filename\": os.path.basename(file_path),\n",
    "                        \"category\": os.path.dirname(file_path).split('/')[-1]\n",
    "                    }\n",
    "                )\n",
    "                self.documents.append(doc)\n",
    "                print(f\"  ‚úì Loaded: {file_path}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  ‚úó Error loading {file_path}: {str(e)}\")\n",
    "        \n",
    "        load_time = time.time() - start_time\n",
    "        print(f\"\\n‚è±Ô∏è Loading time: {load_time:.2f} seconds\")\n",
    "        print(f\"üìÑ Total documents loaded: {len(self.documents)}\")\n",
    "        \n",
    "        return self.documents\n",
    "\n",
    "# Initialize dan load documents\n",
    "loader = DataLoader()\n",
    "documents = loader.load_documents()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c9a495",
   "metadata": {},
   "source": [
    "## 3. Preprocessing - Cleaning dan Normalisasi Teks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a398b6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßπ Preprocessing completed!\n",
      "‚è±Ô∏è Preprocessing time: 0.00 seconds\n",
      "üìÑ Processed 2 documents\n",
      "\n",
      "üìù Sample processed text (first 300 chars):\n",
      "--- PAGE 1 --- UNIVERSITAS Telkom Ujian Akhir Semester (Final Exam) Ganjil TA. 20252026 (1st Semester, Academic Year 20252026) CAK1BAB3-Algoritma dan Pemrograman 1 (Algorithm and Programming 1) Senin, 5 Januari 2026, Jam 14:00-16:00 (120 menit) (Monday, January 5, 2026, 14:00-16:00 120 minutes) Tim \n"
     ]
    }
   ],
   "source": [
    "class TextPreprocessor:\n",
    "    \"\"\"\n",
    "    Text Preprocessing untuk cleaning dan normalisasi\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def clean_text(text: str) -> str:\n",
    "        \"\"\"\n",
    "        Membersihkan teks dari noise\n",
    "        \"\"\"\n",
    "        # Hapus multiple whitespaces\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        \n",
    "        # Hapus special characters yang tidak perlu (keep alphanumeric, punctuation)\n",
    "        text = re.sub(r'[^\\w\\s\\.,!?;:()\\-\\'\\\"]', '', text)\n",
    "        \n",
    "        # Normalize line breaks\n",
    "        text = text.replace('\\n\\n\\n', '\\n\\n')\n",
    "        \n",
    "        # Strip leading/trailing whitespace\n",
    "        text = text.strip()\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    @staticmethod\n",
    "    def normalize_text(text: str) -> str:\n",
    "        \"\"\"\n",
    "        Normalisasi teks (lowercase, dll)\n",
    "        \"\"\"\n",
    "        # Lowercase (optional - tergantung kebutuhan)\n",
    "        # Untuk educational content, kita pertahankan case untuk proper nouns\n",
    "        \n",
    "        # Remove extra spaces\n",
    "        text = ' '.join(text.split())\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    @staticmethod\n",
    "    def preprocess_documents(documents: List[Document]) -> List[Document]:\n",
    "        \"\"\"\n",
    "        Preprocess list of documents\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        processed_docs = []\n",
    "        \n",
    "        for doc in documents:\n",
    "            # Clean and normalize\n",
    "            cleaned_text = TextPreprocessor.clean_text(doc.page_content)\n",
    "            normalized_text = TextPreprocessor.normalize_text(cleaned_text)\n",
    "            \n",
    "            # Create new document with processed text\n",
    "            processed_doc = Document(\n",
    "                page_content=normalized_text,\n",
    "                metadata=doc.metadata\n",
    "            )\n",
    "            processed_docs.append(processed_doc)\n",
    "        \n",
    "        preprocess_time = time.time() - start_time\n",
    "        print(f\"üßπ Preprocessing completed!\")\n",
    "        print(f\"‚è±Ô∏è Preprocessing time: {preprocess_time:.2f} seconds\")\n",
    "        print(f\"üìÑ Processed {len(processed_docs)} documents\")\n",
    "        \n",
    "        return processed_docs\n",
    "\n",
    "# Preprocess documents\n",
    "preprocessor = TextPreprocessor()\n",
    "processed_documents = preprocessor.preprocess_documents(documents)\n",
    "\n",
    "# Show sample\n",
    "if processed_documents:\n",
    "    print(f\"\\nüìù Sample processed text (first 300 chars):\")\n",
    "    print(processed_documents[0].page_content[:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5f0c70",
   "metadata": {},
   "source": [
    "## 4. Chunking Strategy - Memecah Dokumen dengan Overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63dabe46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÇÔ∏è Chunking completed!\n",
      "‚è±Ô∏è Chunking time: 0.00 seconds\n",
      "üìÑ Original documents: 2\n",
      "üì¶ Total chunks created: 55\n",
      "üìä Average chunks per document: 27.5\n",
      "\n",
      "üìù Sample chunks:\n",
      "\n",
      "--- Chunk 1 ---\n",
      "Source: raw_data/question_bank/uts2.txt\n",
      "Content: --- PAGE 1 --- UNIVERSITAS Telkom Ujian Akhir Semester (Final Exam) Ganjil TA...\n",
      "Length: 77 chars\n",
      "\n",
      "--- Chunk 2 ---\n",
      "Source: raw_data/question_bank/uts2.txt\n",
      "Content: . 20252026 (1st Semester, Academic Year 20252026) CAK1BAB3-Algoritma dan Pemrograman 1 (Algorithm and Programming 1) Senin, 5 Januari 2026, Jam 14:00-16:00 (120 menit) (Monday, January 5, 2026, 14:00-...\n",
      "Length: 451 chars\n",
      "\n",
      "--- Chunk 3 ---\n",
      "Source: raw_data/question_bank/uts2.txt\n",
      "Content: . Jika dilakukan, maka dianggap pelanggaran (This is a close book exam, no electronic device is allowed. Put your phone and laptop at the front of class Do not cooperate with each other or violate aca...\n",
      "Length: 466 chars\n"
     ]
    }
   ],
   "source": [
    "class DocumentChunker:\n",
    "    \"\"\"\n",
    "    Chunking strategy dengan overlap untuk konteks yang lebih baik\n",
    "    \"\"\"\n",
    "    def __init__(self, chunk_size: int = 500, chunk_overlap: int = 50):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            chunk_size: Ukuran maksimal setiap chunk (dalam karakter)\n",
    "            chunk_overlap: Overlap antar chunk untuk kontinuitas konteks\n",
    "        \"\"\"\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "        \n",
    "        # RecursiveCharacterTextSplitter dari LangChain\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "            length_function=len,\n",
    "            separators=[\"\\n\\n\", \"\\n\", \".\", \"!\", \"?\", \",\", \" \", \"\"]\n",
    "        )\n",
    "    \n",
    "    def chunk_documents(self, documents: List[Document]) -> List[Document]:\n",
    "        \"\"\"\n",
    "        Split documents into chunks\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        chunks = self.text_splitter.split_documents(documents)\n",
    "        \n",
    "        chunk_time = time.time() - start_time\n",
    "        print(f\"‚úÇÔ∏è Chunking completed!\")\n",
    "        print(f\"‚è±Ô∏è Chunking time: {chunk_time:.2f} seconds\")\n",
    "        print(f\"üìÑ Original documents: {len(documents)}\")\n",
    "        print(f\"üì¶ Total chunks created: {len(chunks)}\")\n",
    "        print(f\"üìä Average chunks per document: {len(chunks)/len(documents):.1f}\")\n",
    "        \n",
    "        return chunks\n",
    "\n",
    "# Chunk documents\n",
    "chunker = DocumentChunker(chunk_size=500, chunk_overlap=50)\n",
    "chunks = chunker.chunk_documents(processed_documents)\n",
    "\n",
    "# Show sample chunks\n",
    "if chunks:\n",
    "    print(f\"\\nüìù Sample chunks:\")\n",
    "    for i, chunk in enumerate(chunks[:3]):\n",
    "        print(f\"\\n--- Chunk {i+1} ---\")\n",
    "        print(f\"Source: {chunk.metadata.get('source', 'N/A')}\")\n",
    "        print(f\"Content: {chunk.page_content[:200]}...\")\n",
    "        print(f\"Length: {len(chunk.page_content)} chars\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96f54ba",
   "metadata": {},
   "source": [
    "## 5. Embedding Generation - Menggunakan Qwen3-Embedding (atau alternatif open-source)\n",
    "\n",
    "**Note**: Qwen3-Embedding mungkin memerlukan akses khusus. Sebagai alternatif, kita akan menggunakan model open-source yang powerful seperti `sentence-transformers/all-MiniLM-L6-v2` atau `BAAI/bge-small-en-v1.5` yang compatible dengan sistem gratis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33b5dbc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Loading embedding model: sentence-transformers/all-MiniLM-L6-v2\n",
      "‚úÖ Model loaded successfully in 31.98 seconds\n",
      "\n",
      "üß™ Testing embedding generation...\n",
      "‚úÖ Test successful!\n",
      "üìä Embedding dimension: 384\n",
      "‚è±Ô∏è Inference time: 0.1422 seconds\n",
      "üìà Sample values: [0.02782415971159935, 0.001702569075860083, 0.08005549758672714, 0.04666281118988991, 0.03852206468582153]\n"
     ]
    }
   ],
   "source": [
    "class EmbeddingGenerator:\n",
    "    \"\"\"\n",
    "    Embedding generator menggunakan model open-source\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name: str = \"sentence-transformers/all-MiniLM-L6-v2\"):\n",
    "        \"\"\"\n",
    "        Initialize embedding model\n",
    "        \n",
    "        Available models:\n",
    "        - sentence-transformers/all-MiniLM-L6-v2 (lightweight, fast)\n",
    "        - BAAI/bge-small-en-v1.5 (better quality)\n",
    "        - Qwen/Qwen-VL (jika tersedia)\n",
    "        \"\"\"\n",
    "        print(f\"üîÑ Loading embedding model: {model_name}\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Initialize HuggingFace Embeddings\n",
    "        self.embeddings = HuggingFaceEmbeddings(\n",
    "            model_name=model_name,\n",
    "            model_kwargs={'device': 'cpu'},  # Use 'cuda' if GPU available\n",
    "            encode_kwargs={'normalize_embeddings': True}  # Normalize for better similarity\n",
    "        )\n",
    "        \n",
    "        load_time = time.time() - start_time\n",
    "        print(f\"‚úÖ Model loaded successfully in {load_time:.2f} seconds\")\n",
    "        \n",
    "    def generate_embeddings(self, texts: List[str]) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Generate embeddings untuk list of texts\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        embeddings = self.embeddings.embed_documents(texts)\n",
    "        \n",
    "        embed_time = time.time() - start_time\n",
    "        \n",
    "        return np.array(embeddings), embed_time\n",
    "    \n",
    "    def test_embedding(self, text: str = \"Test embedding generation\"):\n",
    "        \"\"\"\n",
    "        Test embedding generation dengan sample text\n",
    "        \"\"\"\n",
    "        print(f\"\\nüß™ Testing embedding generation...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        embedding = self.embeddings.embed_query(text)\n",
    "        \n",
    "        test_time = time.time() - start_time\n",
    "        \n",
    "        print(f\"‚úÖ Test successful!\")\n",
    "        print(f\"üìä Embedding dimension: {len(embedding)}\")\n",
    "        print(f\"‚è±Ô∏è Inference time: {test_time:.4f} seconds\")\n",
    "        print(f\"üìà Sample values: {embedding[:5]}\")\n",
    "        \n",
    "        return embedding\n",
    "\n",
    "# Initialize embedding generator\n",
    "embedding_generator = EmbeddingGenerator(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    ")\n",
    "\n",
    "# Test embedding\n",
    "test_embedding = embedding_generator.test_embedding(\"This is a test sentence for embedding.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4d1b2e",
   "metadata": {},
   "source": [
    "## 6. FAISS Vector Store - Indexing dan Penyimpanan Efisien"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "43eeebca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî® Creating FAISS index...\n",
      "‚úÖ FAISS index created successfully!\n",
      "‚è±Ô∏è Indexing time: 2.05 seconds\n",
      "üì¶ Total vectors in index: 55\n",
      "üíæ Estimated memory usage: 20.62 KB\n",
      "üíæ Saving FAISS index to faiss_index...\n",
      "‚úÖ Index saved successfully!\n"
     ]
    }
   ],
   "source": [
    "class FAISSVectorStore:\n",
    "    \"\"\"\n",
    "    FAISS Vector Store untuk penyimpanan dan pencarian efisien\n",
    "    \"\"\"\n",
    "    def __init__(self, embeddings):\n",
    "        self.embeddings = embeddings\n",
    "        self.vector_store = None\n",
    "        \n",
    "    def create_index(self, chunks: List[Document]) -> FAISS:\n",
    "        \"\"\"\n",
    "        Create FAISS index dari chunks\n",
    "        \"\"\"\n",
    "        print(f\"üî® Creating FAISS index...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Create FAISS vector store from documents\n",
    "        self.vector_store = FAISS.from_documents(\n",
    "            documents=chunks,\n",
    "            embedding=self.embeddings\n",
    "        )\n",
    "        \n",
    "        index_time = time.time() - start_time\n",
    "        \n",
    "        print(f\"‚úÖ FAISS index created successfully!\")\n",
    "        print(f\"‚è±Ô∏è Indexing time: {index_time:.2f} seconds\")\n",
    "        print(f\"üì¶ Total vectors in index: {len(chunks)}\")\n",
    "        print(f\"üíæ Estimated memory usage: {len(chunks) * 384 / 1024:.2f} KB\")  # 384 = embedding dimension\n",
    "        \n",
    "        return self.vector_store\n",
    "    \n",
    "    def save_index(self, path: str = \"faiss_index\"):\n",
    "        \"\"\"\n",
    "        Save FAISS index ke disk untuk reuse\n",
    "        \"\"\"\n",
    "        if self.vector_store is None:\n",
    "            print(\"‚ùå No vector store to save!\")\n",
    "            return\n",
    "        \n",
    "        print(f\"üíæ Saving FAISS index to {path}...\")\n",
    "        self.vector_store.save_local(path)\n",
    "        print(f\"‚úÖ Index saved successfully!\")\n",
    "    \n",
    "    def load_index(self, path: str = \"faiss_index\"):\n",
    "        \"\"\"\n",
    "        Load FAISS index dari disk\n",
    "        \"\"\"\n",
    "        print(f\"üìÇ Loading FAISS index from {path}...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        self.vector_store = FAISS.load_local(\n",
    "            path, \n",
    "            self.embeddings,\n",
    "            allow_dangerous_deserialization=True  # Required for loading\n",
    "        )\n",
    "        \n",
    "        load_time = time.time() - start_time\n",
    "        print(f\"‚úÖ Index loaded successfully in {load_time:.2f} seconds!\")\n",
    "        \n",
    "        return self.vector_store\n",
    "\n",
    "# Create FAISS vector store\n",
    "faiss_store = FAISSVectorStore(embedding_generator.embeddings)\n",
    "vector_store = faiss_store.create_index(chunks)\n",
    "\n",
    "# Save index untuk reuse nanti\n",
    "faiss_store.save_index(\"faiss_index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfbc143a",
   "metadata": {},
   "source": [
    "## 7. Retrieval System - Similarity Search dengan Top-K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8d987e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Searching for: 'algorithm and programming exam questions'\n",
      "‚úÖ Retrieved 3 documents with scores\n",
      "‚è±Ô∏è Retrieval time: 0.0255 seconds\n",
      "  1. Score: 0.8872 | Source: uts2.txt\n",
      "  2. Score: 1.0135 | Source: uts2.txt\n",
      "  3. Score: 1.0949 | Source: uts2.txt\n",
      "\n",
      "üìÑ Retrieved Documents:\n",
      "\n",
      "--- Document 1 (Score: 0.8872) ---\n",
      "Source: raw_data/question_bank/uts2.txt\n",
      "Content preview: . 20252026 (1st Semester, Academic Year 20252026) CAK1BAB3-Algoritma dan Pemrograman 1 (Algorithm and Programming 1) Senin, 5 Januari 2026, Jam 14:00-16:00 (120 menit) (Monday, January 5, 2026, 14:00-...\n",
      "\n",
      "--- Document 2 (Score: 1.0135) ---\n",
      "Source: raw_data/question_bank/uts2.txt\n",
      "Content preview: . Tuliskan jawaban di sini (Write your answer at the given box)! Plaintext program Nature dictionary p, kerdil : real ada : integer algorithm ada 1 input(kerdil) input(p) while p ! -1 do if p kerdil t...\n",
      "\n",
      "--- Document 3 (Score: 1.0949) ---\n",
      "Source: raw_data/question_bank/uts2.txt\n",
      "Content preview: . Students are able to use basic sequential processes in a computational problem-solving solution. Target Capaian Pembelajaran Program Studi (Program Learning Outcome) Kemampuan menerapkan prinsip-pri...\n"
     ]
    }
   ],
   "source": [
    "class RAGRetriever:\n",
    "    \"\"\"\n",
    "    Retrieval system dengan similarity search\n",
    "    \"\"\"\n",
    "    def __init__(self, vector_store: FAISS, k: int = 3):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            vector_store: FAISS vector store\n",
    "            k: Number of top results to retrieve\n",
    "        \"\"\"\n",
    "        self.vector_store = vector_store\n",
    "        self.k = k\n",
    "    \n",
    "    def retrieve(self, query: str, k: int = None) -> Tuple[List[Document], float]:\n",
    "        \"\"\"\n",
    "        Retrieve top-k most relevant documents\n",
    "        \n",
    "        Returns:\n",
    "            Tuple of (documents, retrieval_time)\n",
    "        \"\"\"\n",
    "        if k is None:\n",
    "            k = self.k\n",
    "            \n",
    "        print(f\"\\nüîç Searching for: '{query}'\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Similarity search\n",
    "        results = self.vector_store.similarity_search(query, k=k)\n",
    "        \n",
    "        retrieval_time = time.time() - start_time\n",
    "        \n",
    "        print(f\"‚úÖ Retrieved {len(results)} documents\")\n",
    "        print(f\"‚è±Ô∏è Retrieval time: {retrieval_time:.4f} seconds\")\n",
    "        \n",
    "        return results, retrieval_time\n",
    "    \n",
    "    def retrieve_with_scores(self, query: str, k: int = None) -> Tuple[List[Tuple[Document, float]], float]:\n",
    "        \"\"\"\n",
    "        Retrieve dengan similarity scores\n",
    "        \"\"\"\n",
    "        if k is None:\n",
    "            k = self.k\n",
    "            \n",
    "        print(f\"\\nüîç Searching for: '{query}'\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Similarity search with scores\n",
    "        results = self.vector_store.similarity_search_with_score(query, k=k)\n",
    "        \n",
    "        retrieval_time = time.time() - start_time\n",
    "        \n",
    "        print(f\"‚úÖ Retrieved {len(results)} documents with scores\")\n",
    "        print(f\"‚è±Ô∏è Retrieval time: {retrieval_time:.4f} seconds\")\n",
    "        \n",
    "        # Display scores\n",
    "        for i, (doc, score) in enumerate(results):\n",
    "            print(f\"  {i+1}. Score: {score:.4f} | Source: {doc.metadata.get('filename', 'N/A')}\")\n",
    "        \n",
    "        return results, retrieval_time\n",
    "\n",
    "# Initialize retriever\n",
    "retriever = RAGRetriever(vector_store, k=3)\n",
    "\n",
    "# Test retrieval\n",
    "test_query = \"algorithm and programming exam questions\"\n",
    "test_results, test_time = retriever.retrieve_with_scores(test_query)\n",
    "\n",
    "# Show retrieved documents\n",
    "print(f\"\\nüìÑ Retrieved Documents:\")\n",
    "for i, (doc, score) in enumerate(test_results):\n",
    "    print(f\"\\n--- Document {i+1} (Score: {score:.4f}) ---\")\n",
    "    print(f\"Source: {doc.metadata.get('source', 'N/A')}\")\n",
    "    print(f\"Content preview: {doc.page_content[:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2c2f59",
   "metadata": {},
   "source": [
    "## 8. Teacher Agent Input Handler - Student Profile Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc73bbf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TeacherAgentInput:\n",
    "    \"\"\"\n",
    "    Handler untuk input dari Teacher Agent\n",
    "    \"\"\"\n",
    "    def __init__(self, retriever: RAGRetriever):\n",
    "        self.retriever = retriever\n",
    "    \n",
    "    def process_student_input(\n",
    "        self, \n",
    "        student_input: str, \n",
    "        summary: str = \"\", \n",
    "        student_profile: Dict[str, Any] = None\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Process student input dan retrieve relevant context\n",
    "        \n",
    "        Args:\n",
    "            student_input: Input jawaban dari siswa\n",
    "            summary: Summary dari jawaban siswa\n",
    "            student_profile: Profile siswa (level, history, dll)\n",
    "            \n",
    "        Returns:\n",
    "            Dict dengan context dan metadata\n",
    "        \"\"\"\n",
    "        print(\"=\"*60)\n",
    "        print(\"üéì TEACHER AGENT - RAG SYSTEM\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Build enhanced query\n",
    "        query_parts = []\n",
    "        \n",
    "        if summary:\n",
    "            query_parts.append(f\"Summary: {summary}\")\n",
    "        \n",
    "        query_parts.append(f\"Student Input: {student_input}\")\n",
    "        \n",
    "        if student_profile:\n",
    "            level = student_profile.get('level', 'N/A')\n",
    "            query_parts.append(f\"Level: {level}\")\n",
    "        \n",
    "        enhanced_query = \" | \".join(query_parts)\n",
    "        \n",
    "        print(f\"\\nüìù Student Input: {student_input[:100]}...\")\n",
    "        if summary:\n",
    "            print(f\"üìã Summary: {summary[:100]}...\")\n",
    "        if student_profile:\n",
    "            print(f\"üë§ Student Profile: {student_profile}\")\n",
    "        \n",
    "        # Retrieve relevant context\n",
    "        results, retrieval_time = self.retriever.retrieve_with_scores(\n",
    "            enhanced_query, \n",
    "            k=3\n",
    "        )\n",
    "        \n",
    "        # Format context\n",
    "        context_text = self._format_context(results)\n",
    "        \n",
    "        # Build output\n",
    "        output = {\n",
    "            \"context\": context_text,\n",
    "            \"student_input\": student_input,\n",
    "            \"summary\": summary,\n",
    "            \"student_profile\": student_profile,\n",
    "            \"retrieval_time\": retrieval_time,\n",
    "            \"num_sources\": len(results),\n",
    "            \"sources\": [\n",
    "                {\n",
    "                    \"filename\": doc.metadata.get('filename', 'N/A'),\n",
    "                    \"category\": doc.metadata.get('category', 'N/A'),\n",
    "                    \"score\": float(score),\n",
    "                    \"preview\": doc.page_content[:150]\n",
    "                }\n",
    "                for doc, score in results\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def _format_context(self, results: List[Tuple[Document, float]]) -> str:\n",
    "        \"\"\"\n",
    "        Format retrieved documents menjadi context string\n",
    "        \"\"\"\n",
    "        context_parts = []\n",
    "        \n",
    "        for i, (doc, score) in enumerate(results):\n",
    "            context_parts.append(f\"[Source {i+1} - Score: {score:.4f}]\")\n",
    "            context_parts.append(doc.page_content)\n",
    "            context_parts.append(\"\")  # Empty line\n",
    "        \n",
    "        return \"\\n\".join(context_parts)\n",
    "    \n",
    "    def format_output_for_agents(self, output: Dict[str, Any]) -> str:\n",
    "        \"\"\"\n",
    "        Format output dalam bentuk yang siap digunakan oleh downstream agents\n",
    "        (Style Checker, Logic Checker, dll)\n",
    "        \"\"\"\n",
    "        formatted = f\"\"\"\n",
    "{{\n",
    "    \"context\": \"{output['context'][:500]}...\",\n",
    "    \"student_input\": \"{output['student_input']}\",\n",
    "    \"summary\": \"{output['summary']}\",\n",
    "    \"metadata\": {{\n",
    "        \"num_sources\": {output['num_sources']},\n",
    "        \"retrieval_time\": {output['retrieval_time']:.4f},\n",
    "        \"student_profile\": {output['student_profile']}\n",
    "    }}\n",
    "}}\n",
    "\"\"\"\n",
    "        return formatted\n",
    "\n",
    "# Initialize Teacher Agent Input Handler\n",
    "teacher_agent = TeacherAgentInput(retriever)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076a113d",
   "metadata": {},
   "source": [
    "## 9. Complete RAG Pipeline - End-to-End System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f58e901",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üöÄ BUILDING COMPLETE RAG PIPELINE\n",
      "============================================================\n",
      "\n",
      "üìÇ Existing FAISS index found. Loading...\n",
      "üîÑ Loading embedding model: sentence-transformers/all-MiniLM-L6-v2\n",
      "‚úÖ Model loaded successfully in 4.67 seconds\n",
      "üìÇ Loading FAISS index from faiss_index...\n",
      "‚úÖ Index loaded successfully in 0.00 seconds!\n",
      "\n",
      "üéØ Initializing retriever and teacher agent...\n",
      "‚úÖ Agents initialized!\n",
      "\n",
      "============================================================\n",
      "‚úÖ PIPELINE BUILD COMPLETE!\n",
      "‚è±Ô∏è Total time: 4.67 seconds\n",
      "============================================================\n",
      "\n",
      "üìä PIPELINE METRICS:\n",
      "  ‚Ä¢ Total Documents: 0\n",
      "  ‚Ä¢ Total Chunks: 0\n",
      "  ‚Ä¢ Embedding Model: sentence-transformers/all-MiniLM-L6-v2\n",
      "  ‚Ä¢ Chunk Size: 500\n",
      "  ‚Ä¢ Top-K: 3\n"
     ]
    }
   ],
   "source": [
    "class CompleteRAGPipeline:\n",
    "    \"\"\"\n",
    "    Complete RAG Pipeline yang mengintegrasikan semua komponen\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        data_dir: str = \"raw_data\",\n",
    "        embedding_model: str = \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "        chunk_size: int = 500,\n",
    "        chunk_overlap: int = 50,\n",
    "        top_k: int = 3\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize complete RAG pipeline\n",
    "        \"\"\"\n",
    "        self.data_dir = data_dir\n",
    "        self.embedding_model = embedding_model\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "        self.top_k = top_k\n",
    "        \n",
    "        # Components\n",
    "        self.loader = None\n",
    "        self.preprocessor = None\n",
    "        self.chunker = None\n",
    "        self.embedding_generator = None\n",
    "        self.faiss_store = None\n",
    "        self.retriever = None\n",
    "        self.teacher_agent = None\n",
    "        \n",
    "        # Metrics\n",
    "        self.metrics = {\n",
    "            \"load_time\": 0,\n",
    "            \"preprocess_time\": 0,\n",
    "            \"chunk_time\": 0,\n",
    "            \"embedding_time\": 0,\n",
    "            \"index_time\": 0,\n",
    "            \"total_documents\": 0,\n",
    "            \"total_chunks\": 0\n",
    "        }\n",
    "    \n",
    "    def build_pipeline(self, force_rebuild: bool = False):\n",
    "        \"\"\"\n",
    "        Build complete pipeline\n",
    "        \"\"\"\n",
    "        print(\"=\"*60)\n",
    "        print(\"üöÄ BUILDING COMPLETE RAG PIPELINE\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        total_start = time.time()\n",
    "        \n",
    "        # Check if index exists\n",
    "        if os.path.exists(\"faiss_index\") and not force_rebuild:\n",
    "            print(\"\\nüìÇ Existing FAISS index found. Loading...\")\n",
    "            self._load_existing_pipeline()\n",
    "        else:\n",
    "            print(\"\\nüî® Building new pipeline from scratch...\")\n",
    "            self._build_from_scratch()\n",
    "        \n",
    "        total_time = time.time() - total_start\n",
    "        self.metrics[\"total_pipeline_time\"] = total_time\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"‚úÖ PIPELINE BUILD COMPLETE!\")\n",
    "        print(f\"‚è±Ô∏è Total time: {total_time:.2f} seconds\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        self._print_metrics()\n",
    "    \n",
    "    def _build_from_scratch(self):\n",
    "        \"\"\"\n",
    "        Build pipeline from scratch\n",
    "        \"\"\"\n",
    "        # 1. Load documents\n",
    "        print(\"\\nüìö Step 1: Loading documents...\")\n",
    "        self.loader = DataLoader(self.data_dir)\n",
    "        documents = self.loader.load_documents()\n",
    "        self.metrics[\"total_documents\"] = len(documents)\n",
    "        \n",
    "        # 2. Preprocess\n",
    "        print(\"\\nüßπ Step 2: Preprocessing...\")\n",
    "        self.preprocessor = TextPreprocessor()\n",
    "        processed_docs = self.preprocessor.preprocess_documents(documents)\n",
    "        \n",
    "        # 3. Chunk\n",
    "        print(\"\\n‚úÇÔ∏è Step 3: Chunking...\")\n",
    "        self.chunker = DocumentChunker(self.chunk_size, self.chunk_overlap)\n",
    "        chunks = self.chunker.chunk_documents(processed_docs)\n",
    "        self.metrics[\"total_chunks\"] = len(chunks)\n",
    "        \n",
    "        # 4. Generate embeddings and create index\n",
    "        print(\"\\nüî¢ Step 4: Generating embeddings...\")\n",
    "        self.embedding_generator = EmbeddingGenerator(self.embedding_model)\n",
    "        \n",
    "        print(\"\\nüóÑÔ∏è Step 5: Creating FAISS index...\")\n",
    "        self.faiss_store = FAISSVectorStore(self.embedding_generator.embeddings)\n",
    "        vector_store = self.faiss_store.create_index(chunks)\n",
    "        \n",
    "        # 6. Save index\n",
    "        print(\"\\nüíæ Step 6: Saving index...\")\n",
    "        self.faiss_store.save_index(\"faiss_index\")\n",
    "        \n",
    "        # 7. Initialize retriever and teacher agent\n",
    "        self._initialize_agents(vector_store)\n",
    "    \n",
    "    def _load_existing_pipeline(self):\n",
    "        \"\"\"\n",
    "        Load existing pipeline from saved index\n",
    "        \"\"\"\n",
    "        # Initialize embedding generator\n",
    "        self.embedding_generator = EmbeddingGenerator(self.embedding_model)\n",
    "        \n",
    "        # Load FAISS index\n",
    "        self.faiss_store = FAISSVectorStore(self.embedding_generator.embeddings)\n",
    "        vector_store = self.faiss_store.load_index(\"faiss_index\")\n",
    "        \n",
    "        # Initialize agents\n",
    "        self._initialize_agents(vector_store)\n",
    "    \n",
    "    def _initialize_agents(self, vector_store):\n",
    "        \"\"\"\n",
    "        Initialize retriever and teacher agent\n",
    "        \"\"\"\n",
    "        print(\"\\nüéØ Initializing retriever and teacher agent...\")\n",
    "        self.retriever = RAGRetriever(vector_store, k=self.top_k)\n",
    "        self.teacher_agent = TeacherAgentInput(self.retriever)\n",
    "        print(\"‚úÖ Agents initialized!\")\n",
    "    \n",
    "    def query(\n",
    "        self, \n",
    "        student_input: str, \n",
    "        summary: str = \"\", \n",
    "        student_profile: Dict[str, Any] = None\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Main query method untuk Teacher Agent\n",
    "        \"\"\"\n",
    "        if self.teacher_agent is None:\n",
    "            raise ValueError(\"Pipeline not built! Call build_pipeline() first.\")\n",
    "        \n",
    "        return self.teacher_agent.process_student_input(\n",
    "            student_input, \n",
    "            summary, \n",
    "            student_profile\n",
    "        )\n",
    "    \n",
    "    def _print_metrics(self):\n",
    "        \"\"\"\n",
    "        Print pipeline metrics\n",
    "        \"\"\"\n",
    "        print(f\"\\nüìä PIPELINE METRICS:\")\n",
    "        print(f\"  ‚Ä¢ Total Documents: {self.metrics.get('total_documents', 'N/A')}\")\n",
    "        print(f\"  ‚Ä¢ Total Chunks: {self.metrics.get('total_chunks', 'N/A')}\")\n",
    "        print(f\"  ‚Ä¢ Embedding Model: {self.embedding_model}\")\n",
    "        print(f\"  ‚Ä¢ Chunk Size: {self.chunk_size}\")\n",
    "        print(f\"  ‚Ä¢ Top-K: {self.top_k}\")\n",
    "\n",
    "# Initialize Complete Pipeline\n",
    "pipeline = CompleteRAGPipeline(\n",
    "    data_dir=\"raw_data\",\n",
    "    embedding_model=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=50,\n",
    "    top_k=3\n",
    ")\n",
    "\n",
    "# Build pipeline\n",
    "pipeline.build_pipeline(force_rebuild=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9530405b",
   "metadata": {},
   "source": [
    "## 10. Demo & Testing - Contoh Penggunaan Sistem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3792d0b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TEST CASE 1: Student Answer about Algorithms\n",
      "================================================================================\n",
      "============================================================\n",
      "üéì TEACHER AGENT - RAG SYSTEM\n",
      "============================================================\n",
      "\n",
      "üìù Student Input: \n",
      "The algorithm I wrote uses a loop to find the maximum value in an array. \n",
      "I iterate through each el...\n",
      "üìã Summary: Student implements a linear search algorithm for finding maximum value...\n",
      "üë§ Student Profile: {'name': 'John Doe', 'level': 'beginner', 'course': 'Algorithm and Programming 1', 'previous_score': 75}\n",
      "\n",
      "üîç Searching for: 'Summary: Student implements a linear search algorithm for finding maximum value | Student Input: \n",
      "The algorithm I wrote uses a loop to find the maximum value in an array. \n",
      "I iterate through each element and compare it with the current maximum.\n",
      " | Level: beginner'\n",
      "‚úÖ Retrieved 3 documents with scores\n",
      "‚è±Ô∏è Retrieval time: 0.0328 seconds\n",
      "  1. Score: 1.1493 | Source: uts2.txt\n",
      "  2. Score: 1.2057 | Source: uts1.txt\n",
      "  3. Score: 1.2133 | Source: uts2.txt\n",
      "\n",
      "üì§ OUTPUT FOR DOWNSTREAM AGENTS:\n",
      "================================================================================\n",
      "Context Length: 1440 characters\n",
      "Number of Sources: 3\n",
      "Retrieval Time: 0.0328 seconds\n",
      "\n",
      "üí° Context Preview (first 500 chars):\n",
      "[Source 1 - Score: 1.1493]\n",
      ". 20252026 (1st Semester, Academic Year 20252026) CAK1BAB3-Algoritma dan Pemrograman 1 (Algorithm and Programming 1) Senin, 5 Januari 2026, Jam 14:00-16:00 (120 menit) (Monday, January 5, 2026, 14:00-16:00 120 minutes) Tim Dosen (Lecturer Team): BMG, FFS, FZD, HMT, IGR, JMT, LDS, MIU, PEY, UAI Ujian bersifat CLOSE ALL, kalkulator diperbolehkan, HP NON AKTIF DAN TIDAK DIGUNAKAN selama ujian Dilarang keras bekerja sama dan melakukan perbuatan curang\n",
      "\n",
      "[Source 2 - Score: 1\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "# Test Case 1: Student jawaban tentang algoritma\n",
    "print(\"=\"*80)\n",
    "print(\"TEST CASE 1: Student Answer about Algorithms\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "student_input_1 = \"\"\"\n",
    "The algorithm I wrote uses a loop to find the maximum value in an array. \n",
    "I iterate through each element and compare it with the current maximum.\n",
    "\"\"\"\n",
    "\n",
    "summary_1 = \"Student implements a linear search algorithm for finding maximum value\"\n",
    "\n",
    "student_profile_1 = {\n",
    "    \"name\": \"John Doe\",\n",
    "    \"level\": \"beginner\",\n",
    "    \"course\": \"Algorithm and Programming 1\",\n",
    "    \"previous_score\": 75\n",
    "}\n",
    "\n",
    "result_1 = pipeline.query(\n",
    "    student_input=student_input_1,\n",
    "    summary=summary_1,\n",
    "    student_profile=student_profile_1\n",
    ")\n",
    "\n",
    "print(f\"\\nüì§ OUTPUT FOR DOWNSTREAM AGENTS:\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Context Length: {len(result_1['context'])} characters\")\n",
    "print(f\"Number of Sources: {result_1['num_sources']}\")\n",
    "print(f\"Retrieval Time: {result_1['retrieval_time']:.4f} seconds\")\n",
    "print(f\"\\nüí° Context Preview (first 500 chars):\")\n",
    "print(result_1['context'][:500])\n",
    "print(f\"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "406b8476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TEST CASE 2: Question about Exam Format\n",
      "================================================================================\n",
      "============================================================\n",
      "üéì TEACHER AGENT - RAG SYSTEM\n",
      "============================================================\n",
      "\n",
      "üìù Student Input: \n",
      "I'm confused about the exam format. Is it open book or closed book? \n",
      "Can we use calculators?\n",
      "...\n",
      "üìã Summary: Student asking about exam rules and regulations...\n",
      "üë§ Student Profile: {'name': 'Jane Smith', 'level': 'intermediate', 'course': 'Algorithm and Programming 1', 'previous_score': 85}\n",
      "\n",
      "üîç Searching for: 'Summary: Student asking about exam rules and regulations | Student Input: \n",
      "I'm confused about the exam format. Is it open book or closed book? \n",
      "Can we use calculators?\n",
      " | Level: intermediate'\n",
      "‚úÖ Retrieved 3 documents with scores\n",
      "‚è±Ô∏è Retrieval time: 0.0232 seconds\n",
      "  1. Score: 1.0638 | Source: uts1.txt\n",
      "  2. Score: 1.0816 | Source: uts1.txt\n",
      "  3. Score: 1.0859 | Source: uts2.txt\n",
      "\n",
      "üì§ OUTPUT FOR DOWNSTREAM AGENTS:\n",
      "================================================================================\n",
      "Context Length: 1450 characters\n",
      "Number of Sources: 3\n",
      "Retrieval Time: 0.0232 seconds\n",
      "\n",
      "üìö Retrieved Sources:\n",
      "\n",
      "  Source 1:\n",
      "    ‚Ä¢ File: uts1.txt\n",
      "    ‚Ä¢ Category: question_bank\n",
      "    ‚Ä¢ Similarity Score: 1.0638\n",
      "    ‚Ä¢ Preview: . I did not commit any form of cheating in this exam. If I am found to have violated the rules, I am willing to receive E for this course andor all co...\n",
      "\n",
      "  Source 2:\n",
      "    ‚Ä¢ File: uts1.txt\n",
      "    ‚Ä¢ Category: question_bank\n",
      "    ‚Ä¢ Similarity Score: 1.0816\n",
      "    ‚Ä¢ Preview: --- PAGE 1 --- UNIVERSITAS Telkom Ujian Tengah Semester MID TERM EXAM Ganjil TA. 20252026 CAK1BAB3-ALGORITMA DAN PEMROGRAMAN 1 Rabu, 5 November 2025, ...\n",
      "\n",
      "  Source 3:\n",
      "    ‚Ä¢ File: uts2.txt\n",
      "    ‚Ä¢ Category: question_bank\n",
      "    ‚Ä¢ Similarity Score: 1.0859\n",
      "    ‚Ä¢ Preview: . Jika dilakukan, maka dianggap pelanggaran (This is a close book exam, no electronic device is allowed. Put your phone and laptop at the front of cla...\n"
     ]
    }
   ],
   "source": [
    "# Test Case 2: Student jawaban tentang ujian\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TEST CASE 2: Question about Exam Format\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "student_input_2 = \"\"\"\n",
    "I'm confused about the exam format. Is it open book or closed book? \n",
    "Can we use calculators?\n",
    "\"\"\"\n",
    "\n",
    "summary_2 = \"Student asking about exam rules and regulations\"\n",
    "\n",
    "student_profile_2 = {\n",
    "    \"name\": \"Jane Smith\",\n",
    "    \"level\": \"intermediate\",\n",
    "    \"course\": \"Algorithm and Programming 1\",\n",
    "    \"previous_score\": 85\n",
    "}\n",
    "\n",
    "result_2 = pipeline.query(\n",
    "    student_input=student_input_2,\n",
    "    summary=summary_2,\n",
    "    student_profile=student_profile_2\n",
    ")\n",
    "\n",
    "print(f\"\\nüì§ OUTPUT FOR DOWNSTREAM AGENTS:\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Context Length: {len(result_2['context'])} characters\")\n",
    "print(f\"Number of Sources: {result_2['num_sources']}\")\n",
    "print(f\"Retrieval Time: {result_2['retrieval_time']:.4f} seconds\")\n",
    "\n",
    "print(f\"\\nüìö Retrieved Sources:\")\n",
    "for i, source in enumerate(result_2['sources']):\n",
    "    print(f\"\\n  Source {i+1}:\")\n",
    "    print(f\"    ‚Ä¢ File: {source['filename']}\")\n",
    "    print(f\"    ‚Ä¢ Category: {source['category']}\")\n",
    "    print(f\"    ‚Ä¢ Similarity Score: {source['score']:.4f}\")\n",
    "    print(f\"    ‚Ä¢ Preview: {source['preview']}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a4b683",
   "metadata": {},
   "source": [
    "## 11. Output Format untuk Downstream Agents (Style Checker, Logic Checker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "572972ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "OUTPUT FORMATS FOR DOWNSTREAM AGENTS\n",
      "================================================================================\n",
      "\n",
      "1Ô∏è‚É£ FORMAT FOR STYLE CHECKER:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "{\n",
      "    \"context\": {\n",
      "        \"retrieved_knowledge\": \"[Source 1 - Score: 1.1493]\n",
      ". 20252026 (1st Semester, Academic Year 20252026) CAK1BAB3-Algoritma dan Pemrograman 1 (Algorithm and Programming 1) Senin, 5 Januari 2026, Jam 14:00-16:00 (120 menit) (Monday, January 5, 2026, 14:00-16:00 120 minutes) Tim Dosen (Lecturer Team): BMG, FFS, FZD, HMT, IGR, JM...\",\n",
      "        \"num_sources\": 3,\n",
      "        \"retrieval_confidence\": \"high\"\n",
      "    },\n",
      "    \"student_submission\": {\n",
      "        \"input\": \"\n",
      "The algorithm I wrote uses a loop to find the maximum value in an array. \n",
      "I iterate through each element and compare it with the current maximum.\n",
      "\",\n",
      "        \"summary\": \"Student implements a linear search algorithm for finding maximum value\"\n",
      "    },\n",
      "    \"metadata\": {\n",
      "        \"student_profile\": {'name': 'John Doe', 'level': 'beginner', 'course': 'Algorithm and Programming 1', 'previous_score': 75},\n",
      "        \"retrieval_time_ms\": 32.80\n",
      "    }\n",
      "}\n",
      "\n",
      "\n",
      "2Ô∏è‚É£ FORMAT FOR LOGIC CHECKER:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "{\n",
      "    \"context\": {\n",
      "        \"relevant_concepts\": \"[Source 1 - Score: 1.1493]\n",
      ". 20252026 (1st Semester, Academic Year 20252026) CAK1BAB3-Algoritma dan Pemrograman 1 (Algorithm and Programming 1) Senin, 5 Januari 2026, Jam 14:00-16:00 (120 menit) (Monday, January 5, 2026, 14:00-16:00 120 minutes) Tim Dosen (Lecturer Team): BMG, FFS, FZD, HMT, IGR, JM...\",\n",
      "        \"source_files\": ['uts2.txt', 'uts1.txt', 'uts2.txt'],\n",
      "        \"confidence_scores\": [1.1492998600006104, 1.205725908279419, 1.2133311033248901]\n",
      "    },\n",
      "    \"student_answer\": \"\n",
      "The algorithm I wrote uses a loop to find the maximum value in an array. \n",
      "I iterate through each element and compare it with the current maximum.\n",
      "\",\n",
      "    \"summary\": \"Student implements a linear search algorithm for finding maximum value\",\n",
      "    \"student_info\": {'name': 'John Doe', 'level': 'beginner', 'course': 'Algorithm and Programming 1', 'previous_score': 75}\n",
      "}\n",
      "\n",
      "\n",
      "3Ô∏è‚É£ FORMAT FOR LLM PROMPT INJECTION:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "You are a Teacher Agent providing feedback to a student.\n",
      "\n",
      "CONTEXT (Retrieved from Knowledge Base):\n",
      "[Source 1 - Score: 1.1493]\n",
      ". 20252026 (1st Semester, Academic Year 20252026) CAK1BAB3-Algoritma dan Pemrograman 1 (Algorithm and Programming 1) Senin, 5 Januari 2026, Jam 14:00-16:00 (120 menit) (Monday, January 5, 2026, 14:00-16:00 120 minutes) Tim Dosen (Lecturer Team): BMG, FFS, FZD, HMT, IGR, JMT, LDS, MIU, PEY, UAI Ujian bersifat CLOSE ALL, kalkulator diperbolehkan, HP NON AKTIF DAN TIDAK DIG...\n"
     ]
    }
   ],
   "source": [
    "def format_for_style_checker(result: Dict[str, Any]) -> str:\n",
    "    \"\"\"\n",
    "    Format output khusus untuk Style Checker Agent\n",
    "    \"\"\"\n",
    "    return f\"\"\"\n",
    "{{\n",
    "    \"context\": {{\n",
    "        \"retrieved_knowledge\": \"{result['context'][:300]}...\",\n",
    "        \"num_sources\": {result['num_sources']},\n",
    "        \"retrieval_confidence\": \"high\"\n",
    "    }},\n",
    "    \"student_submission\": {{\n",
    "        \"input\": \"{result['student_input']}\",\n",
    "        \"summary\": \"{result['summary']}\"\n",
    "    }},\n",
    "    \"metadata\": {{\n",
    "        \"student_profile\": {result['student_profile']},\n",
    "        \"retrieval_time_ms\": {result['retrieval_time'] * 1000:.2f}\n",
    "    }}\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "def format_for_logic_checker(result: Dict[str, Any]) -> str:\n",
    "    \"\"\"\n",
    "    Format output khusus untuk Logic Checker Agent\n",
    "    \"\"\"\n",
    "    return f\"\"\"\n",
    "{{\n",
    "    \"context\": {{\n",
    "        \"relevant_concepts\": \"{result['context'][:300]}...\",\n",
    "        \"source_files\": {[s['filename'] for s in result['sources']]},\n",
    "        \"confidence_scores\": {[s['score'] for s in result['sources']]}\n",
    "    }},\n",
    "    \"student_answer\": \"{result['student_input']}\",\n",
    "    \"summary\": \"{result['summary']}\",\n",
    "    \"student_info\": {result['student_profile']}\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "def format_for_llm_prompt(result: Dict[str, Any]) -> str:\n",
    "    \"\"\"\n",
    "    Format output untuk LLM Prompt injection\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "You are a Teacher Agent providing feedback to a student.\n",
    "\n",
    "CONTEXT (Retrieved from Knowledge Base):\n",
    "{result['context']}\n",
    "\n",
    "STUDENT SUBMISSION:\n",
    "Input: {result['student_input']}\n",
    "Summary: {result['summary']}\n",
    "\n",
    "STUDENT PROFILE:\n",
    "{result['student_profile']}\n",
    "\n",
    "TASK:\n",
    "Based on the context retrieved from the knowledge base and the student's submission, provide:\n",
    "1. Assessment of correctness\n",
    "2. Areas of improvement\n",
    "3. Constructive feedback\n",
    "4. Suggestions for further learning\n",
    "\n",
    "Your feedback should be:\n",
    "- Clear and concise\n",
    "- Encouraging and constructive\n",
    "- Aligned with the course materials (shown in context)\n",
    "- Tailored to the student's level\n",
    "\"\"\"\n",
    "    return prompt\n",
    "\n",
    "# Demo: Format untuk berbagai agents\n",
    "print(\"=\"*80)\n",
    "print(\"OUTPUT FORMATS FOR DOWNSTREAM AGENTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n1Ô∏è‚É£ FORMAT FOR STYLE CHECKER:\")\n",
    "print(\"-\"*80)\n",
    "print(format_for_style_checker(result_1))\n",
    "\n",
    "print(\"\\n2Ô∏è‚É£ FORMAT FOR LOGIC CHECKER:\")\n",
    "print(\"-\"*80)\n",
    "print(format_for_logic_checker(result_1))\n",
    "\n",
    "print(\"\\n3Ô∏è‚É£ FORMAT FOR LLM PROMPT INJECTION:\")\n",
    "print(\"-\"*80)\n",
    "print(format_for_llm_prompt(result_1)[:500] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf00699",
   "metadata": {},
   "source": [
    "## 12. Performance Metrics & Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a4454af7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "üî¨ PERFORMANCE BENCHMARK\n",
      "================================================================================\n",
      "\n",
      "üîç Testing query: 'algorithm and programming exam questions...'\n",
      "  ‚è±Ô∏è Average Embedding Time: 9.52 ms\n",
      "  ‚è±Ô∏è Average Retrieval Time: 6.74 ms\n",
      "  ‚è±Ô∏è Average Total Time: 16.26 ms\n",
      "\n",
      "üîç Testing query: 'student cheating policy and violations...'\n",
      "  ‚è±Ô∏è Average Embedding Time: 6.83 ms\n",
      "  ‚è±Ô∏è Average Retrieval Time: 7.02 ms\n",
      "  ‚è±Ô∏è Average Total Time: 13.85 ms\n",
      "\n",
      "üîç Testing query: 'how to calculate array maximum value...'\n",
      "  ‚è±Ô∏è Average Embedding Time: 7.15 ms\n",
      "  ‚è±Ô∏è Average Retrieval Time: 6.67 ms\n",
      "  ‚è±Ô∏è Average Total Time: 13.81 ms\n",
      "\n",
      "üîç Testing query: 'exam rules and regulations...'\n",
      "  ‚è±Ô∏è Average Embedding Time: 6.78 ms\n",
      "  ‚è±Ô∏è Average Retrieval Time: 6.61 ms\n",
      "  ‚è±Ô∏è Average Total Time: 13.38 ms\n",
      "\n",
      "üîç Testing query: 'programming assignment grading criteria...'\n",
      "  ‚è±Ô∏è Average Embedding Time: 6.43 ms\n",
      "  ‚è±Ô∏è Average Retrieval Time: 6.21 ms\n",
      "  ‚è±Ô∏è Average Total Time: 12.64 ms\n",
      "\n",
      "================================================================================\n",
      "üìä OVERALL STATISTICS\n",
      "================================================================================\n",
      "Total Queries: 5\n",
      "Runs per Query: 5\n",
      "Total Measurements: 25\n",
      "\n",
      "‚è±Ô∏è Embedding Time:\n",
      "  ‚Ä¢ Mean: 7.34 ms\n",
      "  ‚Ä¢ Median: 6.60 ms\n",
      "  ‚Ä¢ Std Dev: 3.11 ms\n",
      "  ‚Ä¢ Min: 5.91 ms\n",
      "  ‚Ä¢ Max: 21.91 ms\n",
      "\n",
      "‚è±Ô∏è Retrieval Time:\n",
      "  ‚Ä¢ Mean: 6.65 ms\n",
      "  ‚Ä¢ Median: 6.59 ms\n",
      "  ‚Ä¢ Std Dev: 0.53 ms\n",
      "  ‚Ä¢ Min: 5.95 ms\n",
      "  ‚Ä¢ Max: 8.26 ms\n",
      "\n",
      "üí° Performance Grade:\n",
      "  üü¢ EXCELLENT (< 50ms)\n"
     ]
    }
   ],
   "source": [
    "import statistics\n",
    "\n",
    "class PerformanceBenchmark:\n",
    "    \"\"\"\n",
    "    Benchmark untuk mengukur performa sistem RAG\n",
    "    \"\"\"\n",
    "    def __init__(self, pipeline: CompleteRAGPipeline):\n",
    "        self.pipeline = pipeline\n",
    "        self.results = []\n",
    "    \n",
    "    def run_benchmark(self, test_queries: List[str], num_runs: int = 5):\n",
    "        \"\"\"\n",
    "        Run benchmark dengan multiple queries\n",
    "        \"\"\"\n",
    "        print(\"=\"*80)\n",
    "        print(\"üî¨ PERFORMANCE BENCHMARK\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        all_embedding_times = []\n",
    "        all_retrieval_times = []\n",
    "        \n",
    "        for query in test_queries:\n",
    "            print(f\"\\nüîç Testing query: '{query[:50]}...'\")\n",
    "            \n",
    "            query_times = []\n",
    "            \n",
    "            for i in range(num_runs):\n",
    "                start_time = time.time()\n",
    "                \n",
    "                # Measure embedding time\n",
    "                embed_start = time.time()\n",
    "                _ = self.pipeline.embedding_generator.embeddings.embed_query(query)\n",
    "                embed_time = time.time() - embed_start\n",
    "                \n",
    "                # Measure retrieval time\n",
    "                retrieve_start = time.time()\n",
    "                _ = self.pipeline.retriever.vector_store.similarity_search(query, k=3)\n",
    "                retrieve_time = time.time() - retrieve_start\n",
    "                \n",
    "                total_time = time.time() - start_time\n",
    "                \n",
    "                query_times.append({\n",
    "                    'embed_time': embed_time,\n",
    "                    'retrieve_time': retrieve_time,\n",
    "                    'total_time': total_time\n",
    "                })\n",
    "                \n",
    "                all_embedding_times.append(embed_time)\n",
    "                all_retrieval_times.append(retrieve_time)\n",
    "            \n",
    "            # Calculate stats for this query\n",
    "            avg_embed = statistics.mean([t['embed_time'] for t in query_times])\n",
    "            avg_retrieve = statistics.mean([t['retrieve_time'] for t in query_times])\n",
    "            avg_total = statistics.mean([t['total_time'] for t in query_times])\n",
    "            \n",
    "            print(f\"  ‚è±Ô∏è Average Embedding Time: {avg_embed*1000:.2f} ms\")\n",
    "            print(f\"  ‚è±Ô∏è Average Retrieval Time: {avg_retrieve*1000:.2f} ms\")\n",
    "            print(f\"  ‚è±Ô∏è Average Total Time: {avg_total*1000:.2f} ms\")\n",
    "        \n",
    "        # Overall statistics\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"üìä OVERALL STATISTICS\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"Total Queries: {len(test_queries)}\")\n",
    "        print(f\"Runs per Query: {num_runs}\")\n",
    "        print(f\"Total Measurements: {len(all_embedding_times)}\")\n",
    "        print(f\"\\n‚è±Ô∏è Embedding Time:\")\n",
    "        print(f\"  ‚Ä¢ Mean: {statistics.mean(all_embedding_times)*1000:.2f} ms\")\n",
    "        print(f\"  ‚Ä¢ Median: {statistics.median(all_embedding_times)*1000:.2f} ms\")\n",
    "        print(f\"  ‚Ä¢ Std Dev: {statistics.stdev(all_embedding_times)*1000:.2f} ms\")\n",
    "        print(f\"  ‚Ä¢ Min: {min(all_embedding_times)*1000:.2f} ms\")\n",
    "        print(f\"  ‚Ä¢ Max: {max(all_embedding_times)*1000:.2f} ms\")\n",
    "        \n",
    "        print(f\"\\n‚è±Ô∏è Retrieval Time:\")\n",
    "        print(f\"  ‚Ä¢ Mean: {statistics.mean(all_retrieval_times)*1000:.2f} ms\")\n",
    "        print(f\"  ‚Ä¢ Median: {statistics.median(all_retrieval_times)*1000:.2f} ms\")\n",
    "        print(f\"  ‚Ä¢ Std Dev: {statistics.stdev(all_retrieval_times)*1000:.2f} ms\")\n",
    "        print(f\"  ‚Ä¢ Min: {min(all_retrieval_times)*1000:.2f} ms\")\n",
    "        print(f\"  ‚Ä¢ Max: {max(all_retrieval_times)*1000:.2f} ms\")\n",
    "        \n",
    "        print(f\"\\nüí° Performance Grade:\")\n",
    "        avg_total = statistics.mean(all_embedding_times) + statistics.mean(all_retrieval_times)\n",
    "        if avg_total < 0.05:\n",
    "            grade = \"üü¢ EXCELLENT (< 50ms)\"\n",
    "        elif avg_total < 0.1:\n",
    "            grade = \"üü° GOOD (< 100ms)\"\n",
    "        elif avg_total < 0.2:\n",
    "            grade = \"üü† ACCEPTABLE (< 200ms)\"\n",
    "        else:\n",
    "            grade = \"üî¥ NEEDS OPTIMIZATION (> 200ms)\"\n",
    "        print(f\"  {grade}\")\n",
    "\n",
    "# Run benchmark\n",
    "benchmark = PerformanceBenchmark(pipeline)\n",
    "\n",
    "test_queries = [\n",
    "    \"algorithm and programming exam questions\",\n",
    "    \"student cheating policy and violations\",\n",
    "    \"how to calculate array maximum value\",\n",
    "    \"exam rules and regulations\",\n",
    "    \"programming assignment grading criteria\"\n",
    "]\n",
    "\n",
    "benchmark.run_benchmark(test_queries, num_runs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9e58e9",
   "metadata": {},
   "source": [
    "## 13. Utility Functions - Save & Load Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5b7e8f0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "üíæ QUICK LOAD EXAMPLE\n",
      "================================================================================\n",
      "üöÄ Quick loading RAG pipeline...\n",
      "============================================================\n",
      "üöÄ BUILDING COMPLETE RAG PIPELINE\n",
      "============================================================\n",
      "\n",
      "üìÇ Existing FAISS index found. Loading...\n",
      "üîÑ Loading embedding model: sentence-transformers/all-MiniLM-L6-v2\n",
      "‚úÖ Model loaded successfully in 4.37 seconds\n",
      "üìÇ Loading FAISS index from faiss_index...\n",
      "‚úÖ Index loaded successfully in 0.00 seconds!\n",
      "\n",
      "üéØ Initializing retriever and teacher agent...\n",
      "‚úÖ Agents initialized!\n",
      "\n",
      "============================================================\n",
      "‚úÖ PIPELINE BUILD COMPLETE!\n",
      "‚è±Ô∏è Total time: 4.37 seconds\n",
      "============================================================\n",
      "\n",
      "üìä PIPELINE METRICS:\n",
      "  ‚Ä¢ Total Documents: 0\n",
      "  ‚Ä¢ Total Chunks: 0\n",
      "  ‚Ä¢ Embedding Model: sentence-transformers/all-MiniLM-L6-v2\n",
      "  ‚Ä¢ Chunk Size: 500\n",
      "  ‚Ä¢ Top-K: 3\n",
      "============================================================\n",
      "üéì TEACHER AGENT - RAG SYSTEM\n",
      "============================================================\n",
      "\n",
      "üìù Student Input: What is the exam policy?...\n",
      "üìã Summary: Student asking about exam rules...\n",
      "\n",
      "üîç Searching for: 'Summary: Student asking about exam rules | Student Input: What is the exam policy?'\n",
      "‚úÖ Retrieved 3 documents with scores\n",
      "‚è±Ô∏è Retrieval time: 0.0260 seconds\n",
      "  1. Score: 0.9343 | Source: uts1.txt\n",
      "  2. Score: 1.0122 | Source: uts2.txt\n",
      "  3. Score: 1.0523 | Source: uts2.txt\n",
      "\n",
      "‚úÖ Pipeline reloaded and working!\n",
      "Context retrieved: 1356 characters\n",
      "Sources: 3\n"
     ]
    }
   ],
   "source": [
    "# Quick load function untuk reuse pipeline\n",
    "def quick_load_pipeline(\n",
    "    embedding_model: str = \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    top_k: int = 3\n",
    ") -> CompleteRAGPipeline:\n",
    "    \"\"\"\n",
    "    Quick load existing pipeline from saved index\n",
    "    \"\"\"\n",
    "    print(\"üöÄ Quick loading RAG pipeline...\")\n",
    "    \n",
    "    pipeline = CompleteRAGPipeline(\n",
    "        embedding_model=embedding_model,\n",
    "        top_k=top_k\n",
    "    )\n",
    "    \n",
    "    pipeline.build_pipeline(force_rebuild=False)\n",
    "    \n",
    "    return pipeline\n",
    "\n",
    "# Example usage\n",
    "print(\"=\"*80)\n",
    "print(\"üíæ QUICK LOAD EXAMPLE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Simulate reloading pipeline (it will use existing index)\n",
    "reloaded_pipeline = quick_load_pipeline()\n",
    "\n",
    "# Test the reloaded pipeline\n",
    "test_result = reloaded_pipeline.query(\n",
    "    student_input=\"What is the exam policy?\",\n",
    "    summary=\"Student asking about exam rules\"\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Pipeline reloaded and working!\")\n",
    "print(f\"Context retrieved: {len(test_result['context'])} characters\")\n",
    "print(f\"Sources: {test_result['num_sources']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e817138",
   "metadata": {},
   "source": [
    "## 15. Summary & Documentation\n",
    "\n",
    "### üìö System Overview\n",
    "\n",
    "Sistem RAG (Retrieval-Augmented Generation) ini dirancang khusus untuk mendukung **Teacher Agent** dalam memberikan feedback otomatis kepada siswa.\n",
    "\n",
    "### üîß Key Components:\n",
    "\n",
    "1. **DataLoader**: Membaca file .txt dari folder `raw_data`\n",
    "2. **TextPreprocessor**: Cleaning dan normalisasi teks\n",
    "3. **DocumentChunker**: Chunking dengan overlap untuk konteks yang lebih baik\n",
    "4. **EmbeddingGenerator**: Generate embeddings menggunakan model open-source\n",
    "5. **FAISSVectorStore**: Penyimpanan dan pencarian vektor efisien\n",
    "6. **RAGRetriever**: Similarity search dengan top-k retrieval\n",
    "7. **TeacherAgentInput**: Handler untuk input student dengan profile integration\n",
    "8. **CompleteRAGPipeline**: End-to-end pipeline integration\n",
    "9. **RAGAPIInterface**: API untuk integrasi dengan agent system\n",
    "\n",
    "### ‚ö° Performance:\n",
    "- **Embedding Time**: < 50ms (rata-rata)\n",
    "- **Retrieval Time**: < 10ms (rata-rata)\n",
    "- **Total Query Time**: < 100ms (rata-rata)\n",
    "- **Memory Efficient**: FAISS optimized indexing\n",
    "- **Scalable**: Dapat handle ratusan dokumen\n",
    "\n",
    "### üì§ Output Format:\n",
    "```python\n",
    "{\n",
    "    \"context\": \"Retrieved relevant context from knowledge base...\",\n",
    "    \"student_input\": \"Student's answer or question...\",\n",
    "    \"summary\": \"Summary of student's work...\",\n",
    "    \"student_profile\": {\"level\": \"beginner\", ...},\n",
    "    \"retrieval_time\": 0.0234,\n",
    "    \"num_sources\": 3,\n",
    "    \"sources\": [...]\n",
    "}\n",
    "```\n",
    "\n",
    "### üîÑ Integration Flow:\n",
    "```\n",
    "Student Input ‚Üí RAG System ‚Üí Context Retrieval ‚Üí Style Checker ‚Üí Logic Checker ‚Üí Feedback Generation\n",
    "```\n",
    "\n",
    "### üí° Usage:\n",
    "```python\n",
    "# Initialize pipeline\n",
    "pipeline = CompleteRAGPipeline()\n",
    "pipeline.build_pipeline()\n",
    "\n",
    "# Query\n",
    "result = pipeline.query(\n",
    "    student_input=\"...\",\n",
    "    summary=\"...\",\n",
    "    student_profile={...}\n",
    ")\n",
    "\n",
    "# Use context for downstream agents\n",
    "context = result['context']\n",
    "```\n",
    "\n",
    "### üéØ Use Cases:\n",
    "1. ‚úÖ Automated grading assistance\n",
    "2. ‚úÖ Personalized feedback generation\n",
    "3. ‚úÖ Context-aware tutoring\n",
    "4. ‚úÖ Style and logic checking\n",
    "5. ‚úÖ Student performance analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2613cc",
   "metadata": {},
   "source": [
    "## üéØ Ready to Use!\n",
    "\n",
    "Sistem RAG sudah siap digunakan! Anda dapat:\n",
    "\n",
    "1. **Run semua cell** untuk build pipeline pertama kali\n",
    "2. **Gunakan `pipeline.query()`** untuk mendapatkan context\n",
    "3. **Integrasikan dengan agent system** menggunakan `RAGAPIInterface`\n",
    "4. **Load pipeline** dengan cepat menggunakan `quick_load_pipeline()`\n",
    "\n",
    "### Next Steps:\n",
    "- Tambahkan lebih banyak dokumen ke folder `raw_data/`\n",
    "- Sesuaikan parameter chunking dan top-k sesuai kebutuhan\n",
    "- Integrasikan dengan Style Checker dan Logic Checker agents\n",
    "- Implementasi LLM untuk feedback generation menggunakan context yang di-retrieve"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
