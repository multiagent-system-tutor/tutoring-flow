{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eksperimen 7: LightOnOCR Experiment\n",
    "\n",
    "This notebook implements the OCR pipeline using **LightOnOCR-2-1B** (Hugging Face) instead of PaddleOCR.\n",
    "LightOnOCR is an end-to-end Vision-Language Model optimized for document transcription.\n",
    "\n",
    "**Key Changes:**\n",
    "- Replaced PaddleOCR with LightOnOCR-2-1B.\n",
    "- Using Hugging Face Transformers for inference.\n",
    "- Compares performance against Ground Truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages if not already installed\n",
    "# %pip install git+https://github.com/huggingface/transformers.git torch torchvision pillow accelerate ollama pandas matplotlib seaborn opencv-python\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import AutoModelForVision2Seq, AutoProcessor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_DIR = r'f:/projek dosen/tutoring/Agentic Multimodal Tutor - SLL/dataset/UTS/SOAL2'\n",
    "IMAGES_DIR = DATASET_DIR\n",
    "GT_DIR = DATASET_DIR\n",
    "\n",
    "# ===================== LIMIT PROCESSING =====================\n",
    "USE_LIMIT = True  # Set to True to limit files for testing\n",
    "LIMIT_COUNT = 10   # Process fewer files for initial test (LightOn might be slower on CPU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CER METRIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def levenshtein_distance(s1, s2):\n",
    "    if len(s1) < len(s2):\n",
    "        return levenshtein_distance(s2, s1)\n",
    "    if len(s2) == 0:\n",
    "        return len(s1)\n",
    "    previous_row = range(len(s2) + 1)\n",
    "    for i, c1 in enumerate(s1):\n",
    "        current_row = [i + 1]\n",
    "        for j, c2 in enumerate(s2):\n",
    "            insertions = previous_row[j + 1] + 1\n",
    "            deletions = current_row[j] + 1\n",
    "            substitutions = previous_row[j] + (c1 != c2)\n",
    "            current_row.append(min(insertions, deletions, substitutions))\n",
    "        previous_row = current_row\n",
    "    return previous_row[-1]\n",
    "\n",
    "def calculate_cer(reference, hypothesis):\n",
    "    if not reference:\n",
    "        return 0.0\n",
    "    ref = \" \".join(reference.split())\n",
    "    hyp = \" \".join(hypothesis.split())\n",
    "    return levenshtein_distance(ref, hyp) / len(ref)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GROUND TRUTH LOADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_ground_truth(filename_base):\n",
    "    path = os.path.join(GT_DIR, f\"{filename_base}.txt\")\n",
    "    if os.path.exists(path):\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            return f.read().strip()\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LIGHTON OCR INITIALIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"lightonai/LightOnOCR-2-1B\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(f\"Initializing LightOnOCR on {DEVICE}...\")\n",
    "try:\n",
    "    processor = AutoProcessor.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "    model = AutoModelForVision2Seq.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    print(\"LightOnOCR Model Loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading LightOnOCR: {e}\")\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM REFINEMENT (Optional)\n",
    "We can still use Qwen to refine, or just use LightOnOCR as the final output since it's an LLM itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_llm_refinement(prompt):\n",
    "    # Run subprocess with robust encoding handling\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            [\"ollama\", \"run\", \"qwen2.5:3b-instruct\"],\n",
    "            input=prompt,\n",
    "            text=True,\n",
    "            capture_output=True,\n",
    "            encoding='utf-8',\n",
    "            errors='replace'\n",
    "        )\n",
    "        if result.returncode != 0:\n",
    "            return None\n",
    "        return result.stdout.strip()\n",
    "    except Exception as e:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATA LOADING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_files = (\n",
    "    glob.glob(os.path.join(IMAGES_DIR, \"*.jpg\")) +\n",
    "    glob.glob(os.path.join(IMAGES_DIR, \"*.png\")) +\n",
    "    glob.glob(os.path.join(IMAGES_DIR, \"*.jpeg\"))\n",
    ")\n",
    "\n",
    "results = []\n",
    "\n",
    "if USE_LIMIT and LIMIT_COUNT > 0:\n",
    "    print(f\"Limiting processing to first {LIMIT_COUNT} images.\")\n",
    "    image_files = image_files[:LIMIT_COUNT]\n",
    "\n",
    "print(f\"Found {len(image_files)} images.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAIN PROCESSING LOOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, image_path in enumerate(image_files):\n",
    "    filename = os.path.basename(image_path)\n",
    "    filename_base = os.path.splitext(filename)[0]\n",
    "    gt_text = read_ground_truth(filename_base)\n",
    "\n",
    "    print(f\"\\nProcessing [{idx+1}/{len(image_files)}]: {filename}...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    # ---------- LIGHTON OCR ----------\n",
    "    try:\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        \n",
    "        # Prepare conversation/prompt for LightOn\n",
    "        # The model is trained to transcribe images directly.\n",
    "        conversation = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"image\"},\n",
    "                    {\"type\": \"text\", \"text\": \"Transcribe this document into Markdown.\"}\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        text_prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\n",
    "        inputs = processor(\n",
    "            text=[text_prompt],\n",
    "            images=[image],\n",
    "            padding=True,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(model.device)\n",
    "\n",
    "        # Generate\n",
    "        generated_ids = model.generate(\n",
    "            **inputs, \n",
    "            max_new_tokens=1024,\n",
    "            do_sample=False  # Greedy typically best for OCR\n",
    "        )\n",
    "        \n",
    "        generated_text = processor.batch_decode(\n",
    "            generated_ids, \n",
    "            skip_special_tokens=True, \n",
    "            clean_up_tokenization_spaces=False\n",
    "        )[0]\n",
    "        \n",
    "        # Clean up output (remove prompt if echo'd, though skip_special_tokens should handle most)\n",
    "        # LightOn output usually follows the prompt. \n",
    "        # For now, we take it as is or split if necessary.\n",
    "        raw_text = generated_text\n",
    "        \n",
    "        # If the model echoes the prompt, we might need to split. \n",
    "        # Usually standard chat templates separate roles.\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  [OCR ERROR] {e}\")\n",
    "        raw_text = \"\"\n",
    "\n",
    "    # ---------- LLM REFINEMENT (Optional) ----------\n",
    "    # LightOn is already an LLM, so 'final_text' can just be 'raw_text'.\n",
    "    # But to match previous experiment structure:\n",
    "    final_text = raw_text\n",
    "    \n",
    "    # Uncomment below to enable secondary refinement if desired\n",
    "    # if raw_text:\n",
    "    #     prompt = f\"Fix formatting errors in this OCR text:\\n\\n{raw_text}\"\n",
    "    #     llm_out = run_llm_refinement(prompt)\n",
    "    #     if llm_out: final_text = llm_out\n",
    "\n",
    "    # ---------- METRICS ----------\n",
    "    elapsed = time.time() - start_time\n",
    "    cer_raw = calculate_cer(gt_text, raw_text)\n",
    "    cer_refined = calculate_cer(gt_text, final_text)\n",
    "\n",
    "    print(\n",
    "        f\"  Length: {len(raw_text)} | \"\n",
    "        f\"CER: {cer_raw:.2%} | \"\n",
    "        f\"Time: {elapsed:.2f}s\"\n",
    "    )\n",
    "\n",
    "    results.append({\n",
    "        \"filename\": filename,\n",
    "        \"time\": elapsed,\n",
    "        \"cer_raw\": cer_raw,\n",
    "        \"cer_refined\": cer_refined,\n",
    "        \"raw_text\": raw_text,\n",
    "        \"final_text\": final_text,\n",
    "        \"ground_truth\": gt_text\n",
    "    })\n",
    "\n",
    "    # Save partial results\n",
    "    if len(results) > 0:\n",
    "        pd.DataFrame(results).to_csv('results/exp7_lighton_results.csv', index=False)\n",
    "\n",
    "print(\"\\nDONE. Total processed:\", len(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================== VISUALIZE METRICS =====================\n",
    "if results:\n",
    "    df = pd.DataFrame(results)\n",
    "    print(f\"Average Time: {df['time'].mean():.4f}s\")\n",
    "    print(f\"Average CER (LightOn): {df['cer_raw'].mean():.2%}\")\n",
    "    \n",
    "    # Visualization\n",
    "    try:\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        sns.barplot(data=df, x='filename', y='cer_raw', palette='viridis')\n",
    "        plt.title('CER per Image (LightOnOCR-2-1B)')\n",
    "        plt.xlabel('Filename')\n",
    "        plt.ylabel('Character Error Rate')\n",
    "        if len(df) > 20: plt.xticks([])\n",
    "        else: plt.xticks(rotation=45, ha='right')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('results/cer_lighton.png')\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print(f\"Error plotting: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
