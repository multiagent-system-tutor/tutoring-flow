{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eksperimen 6: OCR Pipeline (PaddleOCR + Qwen 2.5)\n",
    "\n",
    "This notebook implements the full pipeline for Experiment 4 using PaddleOCR for text extraction and Qwen 2.5 (3B Instruct) for text correction/refinement.\n",
    "\n",
    "**Key Improvements:**\n",
    "- Robust Ollama response parsing.\n",
    "- Incremental CSV saving to prevent data loss.\n",
    "- Full dataset processing loop.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ollama in c:\\projekdosen\\tutoring\\tutoring-flow\\.venv\\lib\\site-packages (0.6.1)\n",
      "Requirement already satisfied: paddleocr in c:\\projekdosen\\tutoring\\tutoring-flow\\.venv\\lib\\site-packages (3.3.2)\n",
      "Requirement already satisfied: opencv-python in c:\\projekdosen\\tutoring\\tutoring-flow\\.venv\\lib\\site-packages (4.13.0.90)\n",
      "Requirement already satisfied: numpy in c:\\projekdosen\\tutoring\\tutoring-flow\\.venv\\lib\\site-packages (2.4.1)\n",
      "Requirement already satisfied: pandas in c:\\projekdosen\\tutoring\\tutoring-flow\\.venv\\lib\\site-packages (2.3.3)\n",
      "Requirement already satisfied: matplotlib in c:\\projekdosen\\tutoring\\tutoring-flow\\.venv\\lib\\site-packages (3.10.8)\n",
      "Requirement already satisfied: seaborn in c:\\projekdosen\\tutoring\\tutoring-flow\\.venv\\lib\\site-packages (0.13.2)\n",
      "Requirement already satisfied: httpx>=0.27 in c:\\projekdosen\\tutoring\\tutoring-flow\\.venv\\lib\\site-packages (from ollama) (0.28.1)\n",
      "Requirement already satisfied: pydantic>=2.9 in c:\\projekdosen\\tutoring\\tutoring-flow\\.venv\\lib\\site-packages (from ollama) (2.12.5)\n",
      "Requirement already satisfied: paddlex<3.4.0,>=3.3.0 in c:\\projekdosen\\tutoring\\tutoring-flow\\.venv\\lib\\site-packages (from paddlex[ocr-core]<3.4.0,>=3.3.0->paddleocr) (3.3.13)\n",
      "Requirement already satisfied: PyYAML>=6 in c:\\projekdosen\\tutoring\\tutoring-flow\\.venv\\lib\\site-packages (from paddleocr) (6.0.2)\n",
      "Requirement already satisfied: requests in c:\\projekdosen\\tutoring\\tutoring-flow\\.venv\\lib\\site-packages (from paddleocr) (2.32.5)\n",
      "Requirement already satisfied: typing-extensions>=4.12 in c:\\projekdosen\\tutoring\\tutoring-flow\\.venv\\lib\\site-packages (from paddleocr) (4.15.0)\n",
      "Requirement already satisfied: aistudio-sdk>=0.3.5 in c:\\projekdosen\\tutoring\\tutoring-flow\\.venv\\lib\\site-packages (from paddlex<3.4.0,>=3.3.0->paddlex[ocr-core]<3.4.0,>=3.3.0->paddleocr) (0.3.8)\n",
      "Requirement already satisfied: chardet in c:\\projekdosen\\tutoring\\tutoring-flow\\.venv\\lib\\site-packages (from paddlex<3.4.0,>=3.3.0->paddlex[ocr-core]<3.4.0,>=3.3.0->paddleocr) (5.2.0)\n",
      "Requirement already satisfied: colorlog in c:\\projekdosen\\tutoring\\tutoring-flow\\.venv\\lib\\site-packages (from paddlex<3.4.0,>=3.3.0->paddlex[ocr-core]<3.4.0,>=3.3.0->paddleocr) (6.10.1)\n",
      "Requirement already satisfied: filelock in c:\\projekdosen\\tutoring\\tutoring-flow\\.venv\\lib\\site-packages (from paddlex<3.4.0,>=3.3.0->paddlex[ocr-core]<3.4.0,>=3.3.0->paddleocr) (3.20.3)\n",
      "Requirement already satisfied: huggingface-hub in c:\\projekdosen\\tutoring\\tutoring-flow\\.venv\\lib\\site-packages (from paddlex<3.4.0,>=3.3.0->paddlex[ocr-core]<3.4.0,>=3.3.0->paddleocr) (1.3.1)\n",
      "Requirement already satisfied: modelscope>=1.28.0 in c:\\projekdosen\\tutoring\\tutoring-flow\\.venv\\lib\\site-packages (from paddlex<3.4.0,>=3.3.0->paddlex[ocr-core]<3.4.0,>=3.3.0->paddleocr) (1.33.0)\n",
      "Requirement already satisfied: packaging in c:\\projekdosen\\tutoring\\tutoring-flow\\.venv\\lib\\site-packages (from paddlex<3.4.0,>=3.3.0->paddlex[ocr-core]<3.4.0,>=3.3.0->paddleocr) (25.0)\n",
      "Requirement already satisfied: pillow in c:\\projekdosen\\tutoring\\tutoring-flow\\.venv\\lib\\site-packages (from paddlex<3.4.0,>=3.3.0->paddlex[ocr-core]<3.4.0,>=3.3.0->paddleocr) (12.1.0)\n",
      "Requirement already satisfied: prettytable in c:\\projekdosen\\tutoring\\tutoring-flow\\.venv\\lib\\site-packages (from paddlex<3.4.0,>=3.3.0->paddlex[ocr-core]<3.4.0,>=3.3.0->paddleocr) (3.17.0)\n",
      "Requirement already satisfied: py-cpuinfo in c:\\projekdosen\\tutoring\\tutoring-flow\\.venv\\lib\\site-packages (from paddlex<3.4.0,>=3.3.0->paddlex[ocr-core]<3.4.0,>=3.3.0->paddleocr) (9.0.0)\n",
      "Requirement already satisfied: ruamel.yaml in c:\\projekdosen\\tutoring\\tutoring-flow\\.venv\\lib\\site-packages (from paddlex<3.4.0,>=3.3.0->paddlex[ocr-core]<3.4.0,>=3.3.0->paddleocr) (0.19.1)\n",
      "Requirement already satisfied: ujson in c:\\projekdosen\\tutoring\\tutoring-flow\\.venv\\lib\\site-packages (from paddlex<3.4.0,>=3.3.0->paddlex[ocr-core]<3.4.0,>=3.3.0->paddleocr) (5.11.0)\n",
      "Requirement already satisfied: imagesize in c:\\projekdosen\\tutoring\\tutoring-flow\\.venv\\lib\\site-packages (from paddlex[ocr-core]<3.4.0,>=3.3.0->paddleocr) (1.4.1)\n",
      "Requirement already satisfied: opencv-contrib-python==4.10.0.84 in c:\\projekdosen\\tutoring\\tutoring-flow\\.venv\\lib\\site-packages (from paddlex[ocr-core]<3.4.0,>=3.3.0->paddleocr) (4.10.0.84)\n",
      "Requirement already satisfied: pyclipper in c:\\projekdosen\\tutoring\\tutoring-flow\\.venv\\lib\\site-packages (from paddlex[ocr-core]<3.4.0,>=3.3.0->paddleocr) (1.4.0)\n",
      "Requirement already satisfied: pypdfium2>=4 in c:\\projekdosen\\tutoring\\tutoring-flow\\.venv\\lib\\site-packages (from paddlex[ocr-core]<3.4.0,>=3.3.0->paddleocr) (5.3.0)\n",
      "Requirement already satisfied: python-bidi in c:\\projekdosen\\tutoring\\tutoring-flow\\.venv\\lib\\site-packages (from paddlex[ocr-core]<3.4.0,>=3.3.0->paddleocr) (0.6.7)\n",
      "Requirement already satisfied: shapely in c:\\projekdosen\\tutoring\\tutoring-flow\\.venv\\lib\\site-packages (from paddlex[ocr-core]<3.4.0,>=3.3.0->paddleocr) (2.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\projekdosen\\tutoring\\tutoring-flow\\.venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\projekdosen\\tutoring\\tutoring-flow\\.venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\projekdosen\\tutoring\\tutoring-flow\\.venv\\lib\\site-packages (from pandas) (2025.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\projekdosen\\tutoring\\tutoring-flow\\.venv\\lib\\site-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\projekdosen\\tutoring\\tutoring-flow\\.venv\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\projekdosen\\tutoring\\tutoring-flow\\.venv\\lib\\site-packages (from matplotlib) (4.61.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\projekdosen\\tutoring\\tutoring-flow\\.venv\\lib\\site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: pyparsing>=3 in c:\\projekdosen\\tutoring\\tutoring-flow\\.venv\\lib\\site-packages (from matplotlib) (3.3.1)\n",
      "Requirement already satisfied: psutil in c:\\projekdosen\\tutoring\\tutoring-flow\\.venv\\lib\\site-packages (from aistudio-sdk>=0.3.5->paddlex<3.4.0,>=3.3.0->paddlex[ocr-core]<3.4.0,>=3.3.0->paddleocr) (7.2.1)\n",
      "Requirement already satisfied: tqdm in c:\\projekdosen\\tutoring\\tutoring-flow\\.venv\\lib\\site-packages (from aistudio-sdk>=0.3.5->paddlex<3.4.0,>=3.3.0->paddlex[ocr-core]<3.4.0,>=3.3.0->paddleocr) (4.67.1)\n",
      "Requirement already satisfied: bce-python-sdk in c:\\projekdosen\\tutoring\\tutoring-flow\\.venv\\lib\\site-packages (from aistudio-sdk>=0.3.5->paddlex<3.4.0,>=3.3.0->paddlex[ocr-core]<3.4.0,>=3.3.0->paddleocr) (0.9.59)\n",
      "Requirement already satisfied: click in c:\\projekdosen\\tutoring\\tutoring-flow\\.venv\\lib\\site-packages (from aistudio-sdk>=0.3.5->paddlex<3.4.0,>=3.3.0->paddlex[ocr-core]<3.4.0,>=3.3.0->paddleocr) (8.3.1)\n",
      "Requirement already satisfied: anyio in c:\\projekdosen\\tutoring\\tutoring-flow\\.venv\\lib\\site-packages (from httpx>=0.27->ollama) (4.12.1)\n",
      "Requirement already satisfied: certifi in c:\\projekdosen\\tutoring\\tutoring-flow\\.venv\\lib\\site-packages (from httpx>=0.27->ollama) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\projekdosen\\tutoring\\tutoring-flow\\.venv\\lib\\site-packages (from httpx>=0.27->ollama) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\projekdosen\\tutoring\\tutoring-flow\\.venv\\lib\\site-packages (from httpx>=0.27->ollama) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\projekdosen\\tutoring\\tutoring-flow\\.venv\\lib\\site-packages (from httpcore==1.*->httpx>=0.27->ollama) (0.16.0)\n",
      "Requirement already satisfied: setuptools in c:\\projekdosen\\tutoring\\tutoring-flow\\.venv\\lib\\site-packages (from modelscope>=1.28.0->paddlex<3.4.0,>=3.3.0->paddlex[ocr-core]<3.4.0,>=3.3.0->paddleocr) (65.5.0)\n",
      "Requirement already satisfied: urllib3>=1.26 in c:\\projekdosen\\tutoring\\tutoring-flow\\.venv\\lib\\site-packages (from modelscope>=1.28.0->paddlex<3.4.0,>=3.3.0->paddlex[ocr-core]<3.4.0,>=3.3.0->paddleocr) (2.6.3)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\projekdosen\\tutoring\\tutoring-flow\\.venv\\lib\\site-packages (from pydantic>=2.9->ollama) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in c:\\projekdosen\\tutoring\\tutoring-flow\\.venv\\lib\\site-packages (from pydantic>=2.9->ollama) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\projekdosen\\tutoring\\tutoring-flow\\.venv\\lib\\site-packages (from pydantic>=2.9->ollama) (0.4.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\projekdosen\\tutoring\\tutoring-flow\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\projekdosen\\tutoring\\tutoring-flow\\.venv\\lib\\site-packages (from requests->paddleocr) (3.4.4)\n",
      "Requirement already satisfied: colorama in c:\\projekdosen\\tutoring\\tutoring-flow\\.venv\\lib\\site-packages (from tqdm->aistudio-sdk>=0.3.5->paddlex<3.4.0,>=3.3.0->paddlex[ocr-core]<3.4.0,>=3.3.0->paddleocr) (0.4.6)\n",
      "Requirement already satisfied: pycryptodome>=3.8.0 in c:\\projekdosen\\tutoring\\tutoring-flow\\.venv\\lib\\site-packages (from bce-python-sdk->aistudio-sdk>=0.3.5->paddlex<3.4.0,>=3.3.0->paddlex[ocr-core]<3.4.0,>=3.3.0->paddleocr) (3.23.0)\n",
      "Requirement already satisfied: future>=0.6.0 in c:\\projekdosen\\tutoring\\tutoring-flow\\.venv\\lib\\site-packages (from bce-python-sdk->aistudio-sdk>=0.3.5->paddlex<3.4.0,>=3.3.0->paddlex[ocr-core]<3.4.0,>=3.3.0->paddleocr) (1.0.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\projekdosen\\tutoring\\tutoring-flow\\.venv\\lib\\site-packages (from huggingface-hub->paddlex<3.4.0,>=3.3.0->paddlex[ocr-core]<3.4.0,>=3.3.0->paddleocr) (2026.1.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in c:\\projekdosen\\tutoring\\tutoring-flow\\.venv\\lib\\site-packages (from huggingface-hub->paddlex<3.4.0,>=3.3.0->paddlex[ocr-core]<3.4.0,>=3.3.0->paddleocr) (1.2.0)\n",
      "Requirement already satisfied: shellingham in c:\\projekdosen\\tutoring\\tutoring-flow\\.venv\\lib\\site-packages (from huggingface-hub->paddlex<3.4.0,>=3.3.0->paddlex[ocr-core]<3.4.0,>=3.3.0->paddleocr) (1.5.4)\n",
      "Requirement already satisfied: typer-slim in c:\\projekdosen\\tutoring\\tutoring-flow\\.venv\\lib\\site-packages (from huggingface-hub->paddlex<3.4.0,>=3.3.0->paddlex[ocr-core]<3.4.0,>=3.3.0->paddleocr) (0.21.1)\n",
      "Requirement already satisfied: wcwidth in c:\\projekdosen\\tutoring\\tutoring-flow\\.venv\\lib\\site-packages (from prettytable->paddlex<3.4.0,>=3.3.0->paddlex[ocr-core]<3.4.0,>=3.3.0->paddleocr) (0.2.14)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\projekdosen\\tutoring\\tutoring-flow\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "\u001b[33mChecking connectivity to the model hosters, this may take a while. To bypass this check, set `DISABLE_MODEL_SOURCE_CHECK` to `True`.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "%pip install ollama paddleocr opencv-python numpy pandas matplotlib seaborn\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "from paddleocr import PaddleOCR\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_DIR = r'C:\\projekdosen\\tutoring\\Agentic Multimodal Tutor - SLL\\dataset\\UTS\\SOAL2'\n",
    "IMAGES_DIR = DATASET_DIR\n",
    "GT_DIR = DATASET_DIR\n",
    "\n",
    "# ===================== LIMIT PROCESSING =====================\n",
    "USE_LIMIT = True  # Set to True to limit the number of processed files\n",
    "LIMIT_COUNT = 50   # Number of files to process if limit is active"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def levenshtein_distance(s1, s2):\n",
    "    if len(s1) < len(s2):\n",
    "        return levenshtein_distance(s2, s1)\n",
    "    if len(s2) == 0:\n",
    "        return len(s1)\n",
    "    previous_row = range(len(s2) + 1)\n",
    "    for i, c1 in enumerate(s1):\n",
    "        current_row = [i + 1]\n",
    "        for j, c2 in enumerate(s2):\n",
    "            insertions = previous_row[j + 1] + 1\n",
    "            deletions = current_row[j] + 1\n",
    "            substitutions = previous_row[j] + (c1 != c2)\n",
    "            current_row.append(min(insertions, deletions, substitutions))\n",
    "        previous_row = current_row\n",
    "    return previous_row[-1]\n",
    "\n",
    "def calculate_cer(reference, hypothesis):\n",
    "    if not reference:\n",
    "        return 0.0\n",
    "    ref = \" \".join(reference.split())\n",
    "    hyp = \" \".join(hypothesis.split())\n",
    "    return levenshtein_distance(ref, hyp) / len(ref)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GROUND TRUTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_ground_truth(filename_base):\n",
    "    path = os.path.join(GT_DIR, f\"{filename_base}.txt\")\n",
    "    if os.path.exists(path):\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            return f.read().strip()\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OCR INIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing PaddleOCR...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NUC\\AppData\\Local\\Temp\\ipykernel_15152\\2190322544.py:2: DeprecationWarning: The parameter `use_angle_cls` has been deprecated and will be removed in the future. Please use `use_textline_orientation` instead.\n",
      "  ocr = PaddleOCR(lang=\"en\", enable_mkldnn=False, use_angle_cls=True)\n",
      "c:\\projekdosen\\tutoring\\tutoring-flow\\.venv\\Lib\\site-packages\\paddle\\utils\\cpp_extension\\extension_utils.py:712: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md\n",
      "  warnings.warn(warning_message)\n",
      "\u001b[32mCreating model: ('PP-LCNet_x1_0_doc_ori', None)\u001b[0m\n",
      "\u001b[32mModel files already exist. Using cached files. To redownload, please delete the directory manually: `C:\\Users\\NUC\\.paddlex\\official_models\\PP-LCNet_x1_0_doc_ori`.\u001b[0m\n",
      "\u001b[32mCreating model: ('UVDoc', None)\u001b[0m\n",
      "\u001b[32mModel files already exist. Using cached files. To redownload, please delete the directory manually: `C:\\Users\\NUC\\.paddlex\\official_models\\UVDoc`.\u001b[0m\n",
      "\u001b[32mCreating model: ('PP-LCNet_x1_0_textline_ori', None)\u001b[0m\n",
      "\u001b[32mModel files already exist. Using cached files. To redownload, please delete the directory manually: `C:\\Users\\NUC\\.paddlex\\official_models\\PP-LCNet_x1_0_textline_ori`.\u001b[0m\n",
      "\u001b[32mCreating model: ('PP-OCRv5_server_det', None)\u001b[0m\n",
      "\u001b[32mModel files already exist. Using cached files. To redownload, please delete the directory manually: `C:\\Users\\NUC\\.paddlex\\official_models\\PP-OCRv5_server_det`.\u001b[0m\n",
      "\u001b[32mCreating model: ('en_PP-OCRv5_mobile_rec', None)\u001b[0m\n",
      "\u001b[32mModel files already exist. Using cached files. To redownload, please delete the directory manually: `C:\\Users\\NUC\\.paddlex\\official_models\\en_PP-OCRv5_mobile_rec`.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "print(\"Initializing PaddleOCR...\")\n",
    "ocr = PaddleOCR(lang=\"en\", enable_mkldnn=False, use_angle_cls=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM CALL (AMAN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================== LLM CALL (ROBUST) =====================\n",
    "def run_llm(prompt):\n",
    "    # Run subprocess with robust encoding handling\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            [\"ollama\", \"run\", \"qwen2.5:3b-instruct\"],\n",
    "            input=prompt,\n",
    "            text=True,\n",
    "            capture_output=True,\n",
    "            encoding='utf-8',       # Ensure UTF-8 for I/O\n",
    "            errors='replace'        # Replace chars that fail to encode/decode (fixes charmap error)\n",
    "        )\n",
    "        if result.returncode != 0:\n",
    "            print(f\"  [LLM ERROR] Exit Code: {result.returncode}\")\n",
    "            print(f\"  [LLM STDERR] {result.stderr[:200]}...\") # Print part of stderr\n",
    "            return None\n",
    "            \n",
    "        return result.stdout.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"  [LLM EXCEPTION] {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FILE LIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Limiting processing to first 50 images.\n",
      "Found 50 images.\n"
     ]
    }
   ],
   "source": [
    "image_files = (\n",
    "    glob.glob(os.path.join(IMAGES_DIR, \"*.jpg\")) +\n",
    "    glob.glob(os.path.join(IMAGES_DIR, \"*.png\")) +\n",
    "    glob.glob(os.path.join(IMAGES_DIR, \"*.jpeg\"))\n",
    ")\n",
    "\n",
    "\n",
    "results = []\n",
    "\n",
    "# Apply Limit if Enabled\n",
    "if USE_LIMIT and LIMIT_COUNT > 0:\n",
    "    print(f\"Limiting processing to first {LIMIT_COUNT} images.\")\n",
    "    image_files = image_files[:LIMIT_COUNT]\n",
    "\n",
    "print(f\"Found {len(image_files)} images.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image_path, method='otsu', temp_filename='temp_preprocessed.jpg'):\n",
    "    \"\"\"\n",
    "    Applies B&W conversion and thresholding, then converts back to RGB.\n",
    "    \"\"\"\n",
    "    img = cv2.imread(image_path)\n",
    "    if img is None: return None\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    if method == 'otsu':\n",
    "        # Otsu's thresholding\n",
    "        _, thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    elif method == 'adaptive_gaussian':\n",
    "        # Adaptive Gaussian Thresholding\n",
    "        thresh = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, \n",
    "                                       cv2.THRESH_BINARY, 11, 2)\n",
    "    elif method == 'adaptive_mean':\n",
    "        # Adaptive Mean Thresholding\n",
    "        thresh = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_MEAN_C, \n",
    "                                       cv2.THRESH_BINARY, 11, 2)\n",
    "    elif method == 'binary':\n",
    "        # Simple Binary Thresholding\n",
    "        _, thresh = cv2.threshold(gray, 127, 255, cv2.THRESH_BINARY)\n",
    "    else:\n",
    "        # Default / None -> Return None to signal 'use original'\n",
    "        return None\n",
    "\n",
    "    # Convert back to RGB (3 channels)\n",
    "    result = cv2.cvtColor(thresh, cv2.COLOR_GRAY2BGR)\n",
    "    \n",
    "    # Save to temp file\n",
    "    cv2.imwrite(temp_filename, result)\n",
    "    return temp_filename\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAIN LOOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting processing with methods: ['none', 'otsu', 'adaptive_gaussian', 'adaptive_mean']\n",
      "\n",
      "Processing [1/50]: if4908_103012400180_nomor2.jpg...\n",
      "  > Method: none\n",
      "    OCR Len: 217 | Raw: 31.22% | Refined: 36.65% | Time: 67.98s\n",
      "  > Method: otsu\n",
      "    OCR Len: 191 | Raw: 42.99% | Refined: 38.91% | Time: 23.40s\n",
      "  > Method: adaptive_gaussian\n",
      "    OCR Len: 7 | Raw: 99.55% | Refined: 99.55% | Time: 7.45s\n",
      "  > Method: adaptive_mean\n",
      "    OCR Len: 177 | Raw: 58.37% | Refined: 78.73% | Time: 29.73s\n",
      "\n",
      "Processing [2/50]: if4908_103012400221_nomor2.jpg...\n",
      "  > Method: none\n",
      "    OCR Len: 93 | Raw: 39.20% | Refined: 13.60% | Time: 21.09s\n",
      "  > Method: otsu\n",
      "    OCR Len: 91 | Raw: 36.00% | Refined: 28.80% | Time: 18.37s\n",
      "  > Method: adaptive_gaussian\n",
      "    OCR Len: 97 | Raw: 44.80% | Refined: 28.00% | Time: 19.49s\n",
      "  > Method: adaptive_mean\n",
      "    OCR Len: 98 | Raw: 45.60% | Refined: 36.80% | Time: 19.79s\n",
      "\n",
      "Processing [3/50]: if4908_103012400283_nomor2.jpg...\n",
      "  > Method: none\n",
      "    OCR Len: 134 | Raw: 0.00% | Refined: 0.00% | Time: 24.00s\n",
      "  > Method: otsu\n",
      "    OCR Len: 59 | Raw: 0.00% | Refined: 0.00% | Time: 19.89s\n",
      "  > Method: adaptive_gaussian\n",
      "    OCR Len: 99 | Raw: 0.00% | Refined: 0.00% | Time: 19.16s\n",
      "  > Method: adaptive_mean\n",
      "    OCR Len: 94 | Raw: 0.00% | Refined: 0.00% | Time: 20.68s\n",
      "\n",
      "Processing [4/50]: if4908_103012400355_nomor2.jpg...\n",
      "  > Method: none\n",
      "    OCR Len: 177 | Raw: 24.60% | Refined: 34.76% | Time: 27.45s\n",
      "  > Method: otsu\n",
      "    OCR Len: 175 | Raw: 30.48% | Refined: 44.92% | Time: 47.88s\n",
      "  > Method: adaptive_gaussian\n",
      "    OCR Len: 127 | Raw: 47.06% | Refined: 49.73% | Time: 41.02s\n",
      "  > Method: adaptive_mean\n",
      "    OCR Len: 137 | Raw: 51.87% | Refined: 54.55% | Time: 40.25s\n",
      "\n",
      "Processing [5/50]: if4908_103012400374_nomor2.jpg...\n",
      "  > Method: none\n",
      "    OCR Len: 89 | Raw: 36.97% | Refined: 40.34% | Time: 22.56s\n",
      "  > Method: otsu\n",
      "    OCR Len: 93 | Raw: 50.42% | Refined: 30.25% | Time: 24.70s\n",
      "  > Method: adaptive_gaussian\n",
      "    OCR Len: 7 | Raw: 94.12% | Refined: 89.92% | Time: 14.74s\n",
      "  > Method: adaptive_mean\n",
      "    OCR Len: 0 | Raw: 100.00% | Refined: 100.00% | Time: 11.70s\n",
      "\n",
      "Processing [6/50]: if4908_103012500007_nomor2.jpg...\n",
      "  > Method: none\n",
      "    OCR Len: 210 | Raw: 35.46% | Refined: 55.38% | Time: 50.54s\n",
      "  > Method: otsu\n",
      "    OCR Len: 159 | Raw: 43.03% | Refined: 44.62% | Time: 24.03s\n",
      "  > Method: adaptive_gaussian\n",
      "    OCR Len: 186 | Raw: 44.22% | Refined: 50.20% | Time: 28.22s\n",
      "  > Method: adaptive_mean\n",
      "    OCR Len: 186 | Raw: 44.62% | Refined: 53.39% | Time: 30.38s\n",
      "\n",
      "Processing [7/50]: if4908_1030125000185_nomor2.jpg...\n",
      "  > Method: none\n",
      "    OCR Len: 177 | Raw: 0.00% | Refined: 0.00% | Time: 42.64s\n",
      "  > Method: otsu\n",
      "    OCR Len: 146 | Raw: 0.00% | Refined: 0.00% | Time: 45.93s\n",
      "  > Method: adaptive_gaussian\n",
      "    OCR Len: 128 | Raw: 0.00% | Refined: 0.00% | Time: 33.85s\n",
      "  > Method: adaptive_mean\n",
      "    OCR Len: 113 | Raw: 0.00% | Refined: 0.00% | Time: 27.59s\n",
      "\n",
      "Processing [8/50]: if4908_103012500030_nomor2.jpg...\n",
      "  > Method: none\n",
      "    OCR Len: 185 | Raw: 51.71% | Refined: 47.86% | Time: 42.17s\n",
      "  > Method: otsu\n",
      "    OCR Len: 110 | Raw: 69.66% | Refined: 63.68% | Time: 25.27s\n",
      "  > Method: adaptive_gaussian\n",
      "    OCR Len: 159 | Raw: 58.12% | Refined: 63.25% | Time: 22.35s\n",
      "  > Method: adaptive_mean\n",
      "    OCR Len: 101 | Raw: 89.74% | Refined: 86.32% | Time: 30.26s\n",
      "\n",
      "Processing [9/50]: if4908_103012500033_nomor2.jpg...\n",
      "  > Method: none\n",
      "    OCR Len: 127 | Raw: 29.20% | Refined: 56.20% | Time: 26.87s\n",
      "  > Method: otsu\n",
      "    OCR Len: 82 | Raw: 57.66% | Refined: 52.55% | Time: 34.33s\n",
      "  > Method: adaptive_gaussian\n",
      "    OCR Len: 81 | Raw: 56.93% | Refined: 65.69% | Time: 19.50s\n",
      "  > Method: adaptive_mean\n",
      "    OCR Len: 31 | Raw: 90.51% | Refined: 55.47% | Time: 21.68s\n",
      "\n",
      "Processing [10/50]: if4908_103012500034_nomor2.jpg...\n",
      "  > Method: none\n",
      "    OCR Len: 111 | Raw: 41.98% | Refined: 38.17% | Time: 38.38s\n",
      "  > Method: otsu\n",
      "    OCR Len: 94 | Raw: 61.83% | Refined: 60.31% | Time: 43.90s\n",
      "  > Method: adaptive_gaussian\n",
      "    OCR Len: 75 | Raw: 62.60% | Refined: 43.51% | Time: 43.04s\n",
      "  > Method: adaptive_mean\n",
      "    OCR Len: 82 | Raw: 60.31% | Refined: 50.38% | Time: 53.50s\n",
      "\n",
      "Processing [11/50]: if4908_103012500043_nomor2.jpg...\n",
      "  > Method: none\n",
      "    OCR Len: 137 | Raw: 0.00% | Refined: 0.00% | Time: 27.61s\n",
      "  > Method: otsu\n",
      "    OCR Len: 98 | Raw: 0.00% | Refined: 0.00% | Time: 34.76s\n",
      "  > Method: adaptive_gaussian\n",
      "    OCR Len: 59 | Raw: 0.00% | Refined: 0.00% | Time: 19.35s\n",
      "  > Method: adaptive_mean\n",
      "    OCR Len: 60 | Raw: 0.00% | Refined: 0.00% | Time: 27.29s\n",
      "\n",
      "Processing [12/50]: if4908_103012500045_nomor2.jpg...\n",
      "  > Method: none\n",
      "    OCR Len: 62 | Raw: 45.83% | Refined: 58.33% | Time: 41.91s\n",
      "  > Method: otsu\n",
      "    OCR Len: 49 | Raw: 79.17% | Refined: 85.42% | Time: 51.96s\n",
      "  > Method: adaptive_gaussian\n",
      "    OCR Len: 57 | Raw: 60.42% | Refined: 48.96% | Time: 37.78s\n",
      "  > Method: adaptive_mean\n",
      "    OCR Len: 66 | Raw: 60.42% | Refined: 61.46% | Time: 35.35s\n",
      "\n",
      "Processing [13/50]: if4908_103012500065_nomor2.jpg...\n",
      "  > Method: none\n",
      "    OCR Len: 151 | Raw: 33.53% | Refined: 33.53% | Time: 46.32s\n",
      "  > Method: otsu\n",
      "    OCR Len: 152 | Raw: 31.14% | Refined: 32.34% | Time: 30.36s\n",
      "  > Method: adaptive_gaussian\n",
      "    OCR Len: 150 | Raw: 34.13% | Refined: 41.92% | Time: 39.94s\n",
      "  > Method: adaptive_mean\n",
      "    OCR Len: 137 | Raw: 39.52% | Refined: 35.93% | Time: 22.81s\n",
      "\n",
      "Processing [14/50]: if4908_103012500069_nomor2.jpg...\n",
      "  > Method: none\n",
      "    OCR Len: 169 | Raw: 25.00% | Refined: 25.00% | Time: 21.87s\n",
      "  > Method: otsu\n",
      "    OCR Len: 160 | Raw: 34.04% | Refined: 30.32% | Time: 21.93s\n",
      "  > Method: adaptive_gaussian\n",
      "    OCR Len: 116 | Raw: 61.70% | Refined: 69.15% | Time: 17.79s\n",
      "  > Method: adaptive_mean\n",
      "    OCR Len: 96 | Raw: 89.36% | Refined: 73.94% | Time: 17.86s\n",
      "\n",
      "Processing [15/50]: if4908_103012500073_nomor2.jpg...\n",
      "  > Method: none\n",
      "    OCR Len: 217 | Raw: 149.02% | Refined: 91.18% | Time: 25.15s\n",
      "  > Method: otsu\n",
      "    OCR Len: 164 | Raw: 126.47% | Refined: 131.37% | Time: 27.57s\n",
      "  > Method: adaptive_gaussian\n",
      "    OCR Len: 5 | Raw: 99.02% | Refined: 99.02% | Time: 6.31s\n",
      "  > Method: adaptive_mean\n",
      "    OCR Len: 61 | Raw: 79.41% | Refined: 82.35% | Time: 11.52s\n",
      "\n",
      "Processing [16/50]: if4908_103012500089_nomor2.jpg...\n",
      "  > Method: none\n",
      "    OCR Len: 142 | Raw: 42.54% | Refined: 42.54% | Time: 17.97s\n",
      "  > Method: otsu\n",
      "    OCR Len: 139 | Raw: 51.38% | Refined: 42.54% | Time: 19.85s\n",
      "  > Method: adaptive_gaussian\n",
      "    OCR Len: 0 | Raw: 100.00% | Refined: 100.00% | Time: 4.23s\n",
      "  > Method: adaptive_mean\n",
      "    OCR Len: 0 | Raw: 100.00% | Refined: 100.00% | Time: 4.51s\n",
      "\n",
      "Processing [17/50]: if4908_103012500097_nomor2.jpg...\n",
      "  > Method: none\n",
      "    OCR Len: 127 | Raw: 33.33% | Refined: 41.67% | Time: 26.00s\n",
      "  > Method: otsu\n",
      "    OCR Len: 130 | Raw: 28.03% | Refined: 27.27% | Time: 23.99s\n",
      "  > Method: adaptive_gaussian\n",
      "    OCR Len: 132 | Raw: 38.64% | Refined: 43.94% | Time: 23.71s\n",
      "  > Method: adaptive_mean\n",
      "    OCR Len: 119 | Raw: 51.52% | Refined: 55.30% | Time: 25.75s\n",
      "\n",
      "Processing [18/50]: if4908_103012500098_nomor2.jpg...\n",
      "  > Method: none\n",
      "    OCR Len: 134 | Raw: 44.44% | Refined: 57.78% | Time: 34.84s\n",
      "  > Method: otsu\n",
      "    OCR Len: 75 | Raw: 62.96% | Refined: 41.48% | Time: 23.75s\n",
      "  > Method: adaptive_gaussian\n",
      "    OCR Len: 37 | Raw: 84.44% | Refined: 51.11% | Time: 21.03s\n",
      "  > Method: adaptive_mean\n",
      "    OCR Len: 17 | Raw: 91.85% | Refined: 92.59% | Time: 16.15s\n",
      "\n",
      "Processing [19/50]: if4908_103012500180_nomor2.jpg...\n",
      "  > Method: none\n",
      "    OCR Len: 117 | Raw: 57.58% | Refined: 51.52% | Time: 17.96s\n",
      "  > Method: otsu\n",
      "    OCR Len: 103 | Raw: 53.03% | Refined: 51.52% | Time: 19.70s\n",
      "  > Method: adaptive_gaussian\n",
      "    OCR Len: 7 | Raw: 96.97% | Refined: 95.45% | Time: 7.64s\n",
      "  > Method: adaptive_mean\n",
      "    OCR Len: 0 | Raw: 100.00% | Refined: 100.00% | Time: 5.19s\n",
      "\n",
      "Processing [20/50]: if4908_103012500185_nomor2.jpg...\n",
      "  > Method: none\n",
      "    OCR Len: 164 | Raw: 62.71% | Refined: 72.32% | Time: 25.35s\n",
      "  > Method: otsu\n",
      "    OCR Len: 145 | Raw: 61.02% | Refined: 61.58% | Time: 28.32s\n",
      "  > Method: adaptive_gaussian\n",
      "    OCR Len: 116 | Raw: 59.89% | Refined: 76.27% | Time: 22.91s\n",
      "  > Method: adaptive_mean\n",
      "    OCR Len: 53 | Raw: 88.70% | Refined: 75.14% | Time: 20.79s\n",
      "\n",
      "Processing [21/50]: if4908_103012500187_nomor2.jpg...\n",
      "  > Method: none\n",
      "    OCR Len: 176 | Raw: 38.92% | Refined: 28.08% | Time: 30.55s\n",
      "  > Method: otsu\n",
      "    OCR Len: 166 | Raw: 41.38% | Refined: 41.87% | Time: 34.42s\n",
      "  > Method: adaptive_gaussian\n",
      "    OCR Len: 161 | Raw: 45.32% | Refined: 43.84% | Time: 33.53s\n",
      "  > Method: adaptive_mean\n",
      "    OCR Len: 127 | Raw: 52.22% | Refined: 51.72% | Time: 29.76s\n",
      "\n",
      "Processing [22/50]: if4908_103012500223_nomor2.jpg...\n",
      "  > Method: none\n",
      "    OCR Len: 139 | Raw: 29.88% | Refined: 35.98% | Time: 25.53s\n",
      "  > Method: otsu\n",
      "    OCR Len: 125 | Raw: 46.34% | Refined: 44.51% | Time: 26.93s\n",
      "  > Method: adaptive_gaussian\n",
      "    OCR Len: 26 | Raw: 92.68% | Refined: 94.51% | Time: 16.11s\n",
      "  > Method: adaptive_mean\n",
      "    OCR Len: 31 | Raw: 92.68% | Refined: 56.71% | Time: 17.81s\n",
      "\n",
      "Processing [23/50]: if4908_103012500227_nomor2.jpg...\n",
      "  > Method: none\n",
      "    OCR Len: 181 | Raw: 41.26% | Refined: 31.39% | Time: 24.43s\n",
      "  > Method: otsu\n",
      "    OCR Len: 156 | Raw: 62.33% | Refined: 64.57% | Time: 41.82s\n",
      "  > Method: adaptive_gaussian\n",
      "    OCR Len: 7 | Raw: 98.21% | Refined: 98.21% | Time: 21.95s\n",
      "  > Method: adaptive_mean\n",
      "    OCR Len: 86 | Raw: 75.34% | Refined: 61.88% | Time: 32.29s\n",
      "\n",
      "Processing [24/50]: if4908_103012500232_nomor2.jpg...\n",
      "  > Method: none\n",
      "    OCR Len: 215 | Raw: 0.00% | Refined: 0.00% | Time: 38.25s\n",
      "  > Method: otsu\n",
      "    OCR Len: 179 | Raw: 0.00% | Refined: 0.00% | Time: 37.30s\n",
      "  > Method: adaptive_gaussian\n",
      "    OCR Len: 83 | Raw: 0.00% | Refined: 0.00% | Time: 17.05s\n",
      "  > Method: adaptive_mean\n",
      "    OCR Len: 104 | Raw: 0.00% | Refined: 0.00% | Time: 29.80s\n",
      "\n",
      "Processing [25/50]: if4908_103012500281_nomor2.jpg...\n",
      "  > Method: none\n",
      "    OCR Len: 108 | Raw: 43.70% | Refined: 24.44% | Time: 27.98s\n",
      "  > Method: otsu\n",
      "    OCR Len: 103 | Raw: 52.59% | Refined: 53.33% | Time: 24.38s\n",
      "  > Method: adaptive_gaussian\n"
     ]
    }
   ],
   "source": [
    "# Create results directories\n",
    "os.makedirs(r'results/preprocessed', exist_ok=True)\n",
    "os.makedirs(r'results/bbox_visualizations', exist_ok=True)\n",
    "\n",
    "# EXPERIMENT SETTINGS\n",
    "PREPROCESS_METHODS = ['none', 'otsu', 'adaptive_gaussian', 'adaptive_mean']\n",
    "results = []\n",
    "\n",
    "print(f\"Starting processing with methods: {PREPROCESS_METHODS}\")\n",
    "\n",
    "for idx, image_path in enumerate(image_files):\n",
    "    filename = os.path.basename(image_path)\n",
    "    filename_base = os.path.splitext(filename)[0]\n",
    "    gt_text = read_ground_truth(filename_base)\n",
    "\n",
    "    print(f\"\\nProcessing [{idx+1}/{len(image_files)}]: {filename}...\")\n",
    "    \n",
    "    for method in PREPROCESS_METHODS:\n",
    "        print(f\"  > Method: {method}\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # ---------- PREPROCESS & SAVE ----------\n",
    "        if method == 'none':\n",
    "            input_path = image_path\n",
    "            # No extra saving for 'none', it's the original image\n",
    "        else:\n",
    "            # Save preprocessed image permanently\n",
    "            preproc_name = f\"{filename_base}_{method}.jpg\"\n",
    "            save_path = os.path.join(r'results/preprocessed', preproc_name)\n",
    "            \n",
    "            input_path = preprocess_image(image_path, method=method, temp_filename=save_path)\n",
    "            if input_path is None:\n",
    "                input_path = image_path # Fallback\n",
    "\n",
    "        # ---------- OCR ----------\n",
    "        try:\n",
    "             # Using input_path (either original or preprocessed file)\n",
    "            ocr_result = ocr.predict(input_path)\n",
    "        except Exception as e:\n",
    "            print(f\"    [OCR ERROR] {e}\")\n",
    "            ocr_result = []\n",
    "            \n",
    "        extracted_lines = []\n",
    "        bboxes = []\n",
    "\n",
    "        if ocr_result and len(ocr_result) > 0:\n",
    "            if isinstance(ocr_result[0], dict) and \"rec_texts\" in ocr_result[0]:\n",
    "                extracted_lines = ocr_result[0][\"rec_texts\"]\n",
    "                if \"dt_polys\" in ocr_result[0]:\n",
    "                    bboxes = ocr_result[0][\"dt_polys\"]\n",
    "            elif isinstance(ocr_result[0], list):\n",
    "                for line in ocr_result[0]:\n",
    "                    if isinstance(line, list) and len(line) >= 2:\n",
    "                        if isinstance(line[1], (tuple, list)):\n",
    "                            extracted_lines.append(line[1][0])\n",
    "                        if isinstance(line[0], list):\n",
    "                            bboxes.append(line[0])\n",
    "        \n",
    "        # ---------- VISUALIZATION (BBOX) ----------\n",
    "        if bboxes:\n",
    "            # Load the ACTUAL image used (Preprocessed or Original) to draw on\n",
    "            img_vis = cv2.imread(input_path)\n",
    "            if img_vis is not None:\n",
    "                # Draw each box\n",
    "                for box in bboxes:\n",
    "                    box = np.array(box).astype(np.int32).reshape((-1, 1, 2))\n",
    "                    cv2.polylines(img_vis, [box], True, (0, 0, 255), 2) # Red BBox\n",
    "                \n",
    "                # Save Visualization\n",
    "                vis_name = f\"vis_{filename_base}_{method}.jpg\"\n",
    "                vis_path = os.path.join(r'results/bbox_visualizations', vis_name)\n",
    "                cv2.imwrite(vis_path, img_vis)\n",
    "\n",
    "        raw_text = \"\\n\".join(extracted_lines)\n",
    "\n",
    "        # ---------- LLM ----------\n",
    "        final_text = raw_text\n",
    "        if raw_text.strip():\n",
    "            if not os.path.exists(\"prompt_correction.txt\"):\n",
    "                print(\"  [ERROR] prompt_correction.txt not found!\")\n",
    "                continue\n",
    "            with open(\"prompt_correction.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "                prompt = f.read().replace(\"{OCR_TEXT}\", raw_text)\n",
    "\n",
    "            # print(\"  [LLM] running...\")\n",
    "            llm_out = run_llm(prompt)\n",
    "\n",
    "            if llm_out is None:\n",
    "                final_text = raw_text\n",
    "            else:\n",
    "                final_text = (\n",
    "                    llm_out\n",
    "                    .replace(\"```plaintext\", \"\")\n",
    "                    .replace(\"```\", \"\")\n",
    "                    .strip()\n",
    "                )\n",
    "\n",
    "        # ---------- METRIC ----------\n",
    "        elapsed = time.time() - start_time\n",
    "        cer_raw = calculate_cer(gt_text, raw_text)\n",
    "        cer_refined = calculate_cer(gt_text, final_text)\n",
    "\n",
    "        print(\n",
    "            f\"    OCR Len: {len(raw_text)} | \"\n",
    "            f\"Raw: {cer_raw:.2%} | \"\n",
    "            f\"Refined: {cer_refined:.2%} | \"\n",
    "            f\"Time: {elapsed:.2f}s\"\n",
    "        )\n",
    "\n",
    "        results.append({\n",
    "            \"filename\": filename,\n",
    "            \"method\": method,\n",
    "            \"time\": elapsed,\n",
    "            \"cer_raw\": cer_raw,\n",
    "            \"cer_refined\": cer_refined,\n",
    "            \"raw_text\": raw_text,\n",
    "            \"final_text\": final_text,\n",
    "            \"ground_truth\": gt_text\n",
    "        })\n",
    "\n",
    "    # Save partial results incrementally\n",
    "    if len(results) > 0:\n",
    "        pd.DataFrame(results).to_csv('results/exp6_changerimage_results.csv', index=False)\n",
    "\n",
    "print(\"\\nDONE. Total processed results:\", len(results))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================== VISUALIZE METRICS =====================\n",
    "if results:\n",
    "    df = pd.DataFrame(results)\n",
    "    print(f\"Average Time: {df['time'].mean():.4f}s\")\n",
    "    \n",
    "    try:\n",
    "        plt.figure(figsize=(14, 7))\n",
    "        # Plot Refined CER for each method\n",
    "        sns.barplot(data=df, x='filename', y='cer_refined', hue='method', palette='viridis')\n",
    "        plt.title('Comparison of Refined CER by Preprocessing Method')\n",
    "        plt.xlabel('Filename')\n",
    "        plt.ylabel('Character Error Rate (0.0 - 1.0)')\n",
    "        \n",
    "        if len(df['filename'].unique()) > 20:\n",
    "            plt.xticks([]) # Hide x labels if too many\n",
    "        else:\n",
    "            plt.xticks(rotation=90, ha='right')\n",
    "            \n",
    "        plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
    "        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        save_path = 'results/exp6_method_comparison.png'\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"Graph saved to {save_path}\")\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print(f\"Error plotting: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
